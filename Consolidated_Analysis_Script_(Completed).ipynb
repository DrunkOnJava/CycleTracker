{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrunkOnJava/CycleTracker/blob/main/Consolidated_Analysis_Script_(Completed).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Consolidated Analysis Script for Emergency Backup Recovery\n",
        "Version: 1.0.0\n",
        "\n",
        "Combines file analysis, message analysis, visualization, and reporting\n",
        "into a single, robust script.\n",
        "\n",
        "Features:\n",
        "- Comprehensive file analysis (hashing, MIME types, patterns)\n",
        "- Detailed message analysis (contacts, conversations, sentiment, topics)\n",
        "- Data visualization generation (distributions, networks)\n",
        "- Automated report generation (summary, category, contact, patterns)\n",
        "- Checkpointing and resume capability\n",
        "- Parallel processing support\n",
        "- Enhanced logging and error handling\n",
        "- System monitoring integration (state file, PID)\n",
        "- Optional dependency handling\n",
        "- Data integrity checks (checksums)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import hashlib\n",
        "import sqlite3\n",
        "import argparse\n",
        "import logging\n",
        "import logging.handlers\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import shutil\n",
        "import traceback\n",
        "import tempfile\n",
        "import signal\n",
        "import platform\n",
        "import uuid\n",
        "import threading\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Any, Set, Tuple, Union, Iterator, Callable\n",
        "from collections import defaultdict, Counter\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, field, asdict\n",
        "\n",
        "# --- Optional Dependency Handling ---\n",
        "\n",
        "# File type detection\n",
        "try:\n",
        "    import magic\n",
        "    MAGIC_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MAGIC_AVAILABLE = False\n",
        "    print(\"WARNING: python-magic not found. MIME type detection will be limited. Install with: pip install python-magic\", file=sys.stderr)\n",
        "\n",
        "# Fuzzy hashing\n",
        "try:\n",
        "    import ssdeep\n",
        "    SSDEEP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SSDEEP_AVAILABLE = False\n",
        "    print(\"WARNING: ssdeep not found. Fuzzy hashing disabled. Install with: pip install ssdeep\", file=sys.stderr)\n",
        "\n",
        "# Network analysis and visualization\n",
        "try:\n",
        "    import networkx as nx\n",
        "    NETWORKX_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NETWORKX_AVAILABLE = False\n",
        "    print(\"WARNING: networkx not found. Network graph analysis disabled. Install with: pip install networkx\", file=sys.stderr)\n",
        "\n",
        "try:\n",
        "    # Ensure using a non-interactive backend BEFORE importing pyplot\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg') # Use Agg backend for non-interactive plotting\n",
        "    import matplotlib.pyplot as plt\n",
        "    MATPLOTLIB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MATPLOTLIB_AVAILABLE = False\n",
        "    print(\"WARNING: matplotlib not found. Plotting and visualization disabled. Install with: pip install matplotlib\", file=sys.stderr)\n",
        "\n",
        "# Data manipulation and visualization\n",
        "try:\n",
        "    import pandas as pd\n",
        "    PANDAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PANDAS_AVAILABLE = False\n",
        "    print(\"WARNING: pandas not found. Some data analysis and reporting features disabled. Install with: pip install pandas\", file=sys.stderr)\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    NUMPY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NUMPY_AVAILABLE = False\n",
        "    print(\"WARNING: numpy not found. Some numerical operations and analysis disabled. Install with: pip install numpy\", file=sys.stderr)\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    # Seaborn depends on pandas and matplotlib\n",
        "    SEABORN_AVAILABLE = PANDAS_AVAILABLE and MATPLOTLIB_AVAILABLE and NUMPY_AVAILABLE\n",
        "except ImportError:\n",
        "    SEABORN_AVAILABLE = False\n",
        "    if PANDAS_AVAILABLE and MATPLOTLIB_AVAILABLE and NUMPY_AVAILABLE:\n",
        "        print(\"WARNING: seaborn not found. Advanced plotting disabled. Install with: pip install seaborn\", file=sys.stderr)\n",
        "\n",
        "# Text analysis\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    # from sklearn.cluster import DBSCAN # Not used in provided code, commented out\n",
        "    SKLEARN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SKLEARN_AVAILABLE = False\n",
        "    print(\"WARNING: scikit-learn not found. Topic modeling disabled. Install with: pip install scikit-learn\", file=sys.stderr)\n",
        "\n",
        "# Sentiment analysis\n",
        "try:\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    SENTIMENT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SENTIMENT_AVAILABLE = False\n",
        "    print(\"WARNING: vaderSentiment not found. Sentiment analysis disabled. Install with: pip install vaderSentiment\", file=sys.stderr)\n",
        "\n",
        "# Phone number parsing\n",
        "try:\n",
        "    # Use specific imports to avoid potential namespace conflicts\n",
        "    from phonenumbers import parse as parse_phone, is_valid_number, format_number, PhoneNumberFormat, NumberParseException\n",
        "    PHONENUMBERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PHONENUMBERS_AVAILABLE = False\n",
        "    print(\"WARNING: phonenumbers not found. Phone number validation/formatting disabled. Install with: pip install phonenumbers\", file=sys.stderr)\n",
        "\n",
        "# Word cloud generation\n",
        "try:\n",
        "    from wordcloud import WordCloud\n",
        "    # Wordcloud depends on matplotlib and numpy\n",
        "    WORDCLOUD_AVAILABLE = MATPLOTLIB_AVAILABLE and NUMPY_AVAILABLE\n",
        "except ImportError:\n",
        "    WORDCLOUD_AVAILABLE = False\n",
        "    if MATPLOTLIB_AVAILABLE and NUMPY_AVAILABLE:\n",
        "        print(\"WARNING: wordcloud not found. Word cloud generation disabled. Install with: pip install wordcloud\", file=sys.stderr)\n",
        "\n",
        "# Tabulate for reports\n",
        "try:\n",
        "    from tabulate import tabulate\n",
        "    TABULATE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TABULATE_AVAILABLE = False\n",
        "    print(\"WARNING: tabulate not found. Text-based reports will be basic. Install with: pip install tabulate\", file=sys.stderr)\n",
        "\n",
        "# --- Constants ---\n",
        "SCRIPT_VERSION = \"1.0.0\"\n",
        "DEFAULT_BATCH_SIZE = 1000\n",
        "DEFAULT_MIN_CONVERSATION_FILES = 3 # Adjusted minimum conversation length\n",
        "MAX_RETRIES = 3\n",
        "RETRY_DELAY = 5 # seconds\n",
        "CHECKPOINT_INTERVAL = 300 # seconds between automatic checkpoints (not currently used for auto-save timer)\n",
        "DEFAULT_REPORT_TOP_N = 20 # Default number of items for top lists in reports\n",
        "DEFAULT_GRAPH_TOP_N = 30 # Default number of nodes for graph visualization\n",
        "\n",
        "# --- Data Classes ---\n",
        "\n",
        "@dataclass\n",
        "class FileInfo:\n",
        "    \"\"\"Data class for storing analyzed file information.\"\"\"\n",
        "    id: Optional[int] = None\n",
        "    path: str = \"\"\n",
        "    filename: str = \"\"\n",
        "    extension: str = \"\"\n",
        "    filesize: int = 0\n",
        "    mimetype: str = \"application/octet-stream\"\n",
        "    md5hash: str = \"\"\n",
        "    fuzzyhash: str = \"\"\n",
        "    creation_date: Optional[str] = None # Use Optional[str] for dates\n",
        "    modification_date: Optional[str] = None # Use Optional[str] for dates\n",
        "    category: str = \"Other\"\n",
        "    value_score: float = 0.0\n",
        "    is_duplicate: int = 0\n",
        "    original_id: Optional[int] = None\n",
        "    patterns: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    last_analyzed: Optional[str] = None # Track when analysis was last run\n",
        "    error: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class MessageMetadata:\n",
        "    \"\"\"Data class for message metadata extracted during analysis.\"\"\"\n",
        "    phone_numbers: List[str] = field(default_factory=list)\n",
        "    timestamps: List[str] = field(default_factory=list)\n",
        "    filename: str = \"\"\n",
        "    file_path: str = \"\"\n",
        "    creation_time: Optional[str] = None\n",
        "    modification_time: Optional[str] = None\n",
        "    content: str = \"\"\n",
        "    entities: Dict[str, List[str]] = field(default_factory=lambda: defaultdict(list))\n",
        "    sentiment_score: float = 0.0\n",
        "    error: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class Contact:\n",
        "    \"\"\"Data class for contact information derived from messages.\"\"\"\n",
        "    phone_number: str\n",
        "    message_count: int = 0\n",
        "    first_seen: Optional[str] = None\n",
        "    last_seen: Optional[str] = None\n",
        "    files: Set[int] = field(default_factory=set) # Set of file IDs\n",
        "    common_contacts: Dict[str, int] = field(default_factory=dict) # Other number -> count\n",
        "    confidence: float = 1.0 # Placeholder for future confidence scoring\n",
        "    name_suggestions: List[str] = field(default_factory=list) # Placeholder\n",
        "    normalized_number: str = \"\"\n",
        "    sentiment_score: float = 0.0 # Average sentiment across messages\n",
        "    topics: List[str] = field(default_factory=list)\n",
        "    location_hints: List[str] = field(default_factory=list)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Convert Contact object to a dictionary suitable for JSON serialization.\"\"\"\n",
        "        result = asdict(self)\n",
        "        result['files'] = sorted(list(self.files)) # Convert set to sorted list\n",
        "        return result\n",
        "\n",
        "@dataclass\n",
        "class Conversation:\n",
        "    \"\"\"Data class for conversation information.\"\"\"\n",
        "    participants: List[str] = field(default_factory=list)\n",
        "    message_count: int = 0\n",
        "    first_message: Optional[str] = None\n",
        "    last_message: Optional[str] = None\n",
        "    files: List[Dict[str, Any]] = field(default_factory=list) # List of {'path': str, 'date': str, 'sentiment': float}\n",
        "    sentiment_score: float = 0.0 # Average sentiment across messages\n",
        "    topics: List[str] = field(default_factory=list)\n",
        "    is_group_chat: bool = False\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Convert Conversation object to a dictionary suitable for JSON serialization.\"\"\"\n",
        "        return asdict(self)\n",
        "\n",
        "# --- Configuration Class ---\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Holds configuration parameters for the analysis.\"\"\"\n",
        "    backup_dir: str\n",
        "    base_output_dir: str\n",
        "    sample_rate: float = 1.0\n",
        "    max_files: Optional[int] = None\n",
        "    resume: bool = False\n",
        "    batch_size: int = DEFAULT_BATCH_SIZE\n",
        "    min_conversation_files: int = DEFAULT_MIN_CONVERSATION_FILES\n",
        "    parallel_jobs: int = os.cpu_count() or 1 # Default to number of CPU cores\n",
        "    use_npu: bool = False # Placeholder, actual usage depends on libraries\n",
        "    debug_level: int = 1 # 0=quiet, 1=info, 2=debug\n",
        "    report_top_n: int = DEFAULT_REPORT_TOP_N\n",
        "    graph_top_n: int = DEFAULT_GRAPH_TOP_N\n",
        "\n",
        "    # Derived paths\n",
        "    log_dir: str = field(init=False)\n",
        "    database_dir: str = field(init=False)\n",
        "    data_dir: str = field(init=False)\n",
        "    visualization_dir: str = field(init=False)\n",
        "    report_dir: str = field(init=False) # Added report directory\n",
        "    monitor_dir: str = field(init=False)\n",
        "    tmp_dir: str = field(init=False)\n",
        "    db_path: str = field(init=False)\n",
        "    checkpoint_path: str = field(init=False) # For file_analyzer progress\n",
        "    state_file_path: str = field(init=False)\n",
        "    pid_file_path: str = field(init=False)\n",
        "    contact_data_path: str = field(init=False) # Path for contacts.json\n",
        "    conversation_data_path: str = field(init=False) # Path for conversations.json\n",
        "    contact_conv_dir: str = field(init=False) # Dir for individual contact conversation files\n",
        "    visualization_data_path: str = field(init=False) # Path for visualization data (e.g., counts)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Calculate derived paths after initialization.\"\"\"\n",
        "        self.base_output_dir = os.path.abspath(self.base_output_dir)\n",
        "        self.log_dir = os.path.join(self.base_output_dir, \"Logs\")\n",
        "        self.database_dir = os.path.join(self.base_output_dir, \"Database\")\n",
        "        self.data_dir = os.path.join(self.base_output_dir, \"Data\")\n",
        "        self.visualization_dir = os.path.join(self.base_output_dir, \"Visualizations\")\n",
        "        self.report_dir = os.path.join(self.base_output_dir, \"Reports\") # Report dir path\n",
        "        self.monitor_dir = os.path.join(self.base_output_dir, \"Monitor\")\n",
        "        self.tmp_dir = os.path.join(self.base_output_dir, \"Tmp\")\n",
        "\n",
        "        self.db_path = os.path.join(self.database_dir, \"analysis.db\")\n",
        "        self.checkpoint_path = os.path.join(self.database_dir, \"file_analyzer_checkpoint.json\")\n",
        "        self.state_file_path = os.path.join(self.monitor_dir, \"analysis_state.json\")\n",
        "        self.pid_file_path = os.path.join(self.monitor_dir, \"analysis.pid\")\n",
        "\n",
        "        # Data output paths\n",
        "        self.contact_data_path = os.path.join(self.data_dir, \"contacts.json\")\n",
        "        self.conversation_data_path = os.path.join(self.data_dir, \"conversations.json\")\n",
        "        self.contact_conv_dir = os.path.join(self.data_dir, \"ConversationsByContact\")\n",
        "        self.visualization_data_path = os.path.join(self.data_dir, \"visualization_data.json\") # Added viz data path\n",
        "\n",
        "\n",
        "    def create_directories(self):\n",
        "        \"\"\"Create all necessary output directories.\"\"\"\n",
        "        dirs = [\n",
        "            self.log_dir, self.database_dir, self.data_dir,\n",
        "            self.visualization_dir,\n",
        "            os.path.join(self.visualization_dir, \"Graphs\"),\n",
        "            os.path.join(self.visualization_dir, \"Charts\"),\n",
        "            os.path.join(self.visualization_dir, \"WordClouds\"), # Added WordCloud dir\n",
        "            self.report_dir, # Added report dir\n",
        "            self.monitor_dir, self.tmp_dir,\n",
        "            self.contact_conv_dir # Added contact conversation dir\n",
        "        ]\n",
        "        for dir_path in dirs:\n",
        "            try:\n",
        "                os.makedirs(dir_path, exist_ok=True)\n",
        "            except OSError as e:\n",
        "                # Use print here as logger might not be fully configured yet\n",
        "                print(f\"ERROR: Failed to create directory: {dir_path} - {e}\", file=sys.stderr)\n",
        "                raise # Re-raise the exception to halt execution if critical dirs fail\n",
        "\n",
        "# --- Logging Setup ---\n",
        "\n",
        "# Global logger instance\n",
        "logger = logging.getLogger('consolidated_analyzer')\n",
        "stop_event = threading.Event() # Global event for signaling shutdown\n",
        "\n",
        "def setup_logging(log_dir: str, debug_level: int) -> None:\n",
        "    \"\"\"Set up advanced logging with rotation and multiple handlers.\"\"\"\n",
        "    log_levels = {\n",
        "        0: logging.WARNING, # Quiet\n",
        "        1: logging.INFO,    # Normal\n",
        "        2: logging.DEBUG    # Verbose/Debug\n",
        "    }\n",
        "    log_level = log_levels.get(debug_level, logging.INFO)\n",
        "\n",
        "    log_format = '%(asctime)s - %(name)s - %(levelname)s - [%(module)s:%(lineno)d] - %(message)s'\n",
        "    date_format = '%Y-%m-%d %H:%M:%S'\n",
        "\n",
        "    logger.setLevel(log_level)\n",
        "\n",
        "    # Clear existing handlers to prevent duplication if called multiple times\n",
        "    if logger.handlers:\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    # Console handler\n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setLevel(log_level)\n",
        "    console_formatter = logging.Formatter(log_format, date_format)\n",
        "    console_handler.setFormatter(console_formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    # File handler for all logs (rotating)\n",
        "    log_file = os.path.join(log_dir, 'analysis_full.log')\n",
        "    # Rotate logs, keep 5 backups, max 10MB each\n",
        "    try:\n",
        "        file_handler = logging.handlers.RotatingFileHandler(\n",
        "            log_file, maxBytes=10*1024*1024, backupCount=5, encoding='utf-8'\n",
        "        )\n",
        "        file_handler.setLevel(logging.DEBUG) # Log everything to the file\n",
        "        file_formatter = logging.Formatter(log_format, date_format)\n",
        "        file_handler.setFormatter(file_formatter)\n",
        "        logger.addHandler(file_handler)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to set up rotating log file handler: {e}\", file=sys.stderr)\n",
        "\n",
        "\n",
        "    # File handler specifically for errors\n",
        "    error_log_file = os.path.join(log_dir, 'analysis_errors.log')\n",
        "    try:\n",
        "        error_handler = logging.FileHandler(error_log_file, encoding='utf-8')\n",
        "        error_handler.setLevel(logging.ERROR)\n",
        "        error_formatter = logging.Formatter(log_format, date_format)\n",
        "        error_handler.setFormatter(error_formatter)\n",
        "        logger.addHandler(error_handler)\n",
        "    except Exception as e:\n",
        "         print(f\"ERROR: Failed to set up error log file handler: {e}\", file=sys.stderr)\n",
        "\n",
        "\n",
        "    # Log startup information\n",
        "    logger.info(f\"--- Consolidated Analysis Script v{SCRIPT_VERSION} Starting ---\")\n",
        "    logger.info(f\"Log level set to {logging.getLevelName(log_level)}\")\n",
        "    logger.info(f\"Full logs: {log_file}\")\n",
        "    logger.info(f\"Error logs: {error_log_file}\")\n",
        "\n",
        "    # Log available optional features\n",
        "    optional_features = {\n",
        "        \"python-magic (MIME types)\": MAGIC_AVAILABLE,\n",
        "        \"ssdeep (Fuzzy Hashing)\": SSDEEP_AVAILABLE,\n",
        "        \"NetworkX (Graph Analysis)\": NETWORKX_AVAILABLE,\n",
        "        \"Matplotlib (Plotting)\": MATPLOTLIB_AVAILABLE,\n",
        "        \"Pandas (DataFrames)\": PANDAS_AVAILABLE,\n",
        "        \"NumPy (Numerical Ops)\": NUMPY_AVAILABLE,\n",
        "        \"Seaborn (Adv. Plotting)\": SEABORN_AVAILABLE,\n",
        "        \"Scikit-learn (Text Analysis)\": SKLEARN_AVAILABLE,\n",
        "        \"VADER Sentiment\": SENTIMENT_AVAILABLE,\n",
        "        \"Phonenumbers (Validation)\": PHONENUMBERS_AVAILABLE,\n",
        "        \"WordCloud\": WORDCLOUD_AVAILABLE,\n",
        "        \"Tabulate (Reports)\": TABULATE_AVAILABLE,\n",
        "    }\n",
        "    logger.debug(\"Optional library availability:\")\n",
        "    for name, available in optional_features.items():\n",
        "        logger.debug(f\"- {name}: {'Available' if available else 'Not Available'}\")\n",
        "\n",
        "# --- Utility Functions ---\n",
        "\n",
        "def calculate_checksum(file_path: str) -> Optional[str]:\n",
        "    \"\"\"Calculate SHA256 checksum for a file.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"Cannot calculate checksum, file not found: {file_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        hasher = hashlib.sha256()\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            while chunk := f.read(8192): # Read in 8KB chunks\n",
        "                hasher.update(chunk)\n",
        "        return hasher.hexdigest()\n",
        "    except OSError as e:\n",
        "        logger.error(f\"Error calculating checksum for {file_path}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error calculating checksum for {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def verify_checksum(file_path: str, expected_checksum: str) -> bool:\n",
        "    \"\"\"Verify the checksum of a file against an expected value.\"\"\"\n",
        "    calculated_checksum = calculate_checksum(file_path)\n",
        "    if calculated_checksum is None:\n",
        "        return False # Error occurred during calculation\n",
        "    if calculated_checksum == expected_checksum:\n",
        "        logger.debug(f\"Checksum verified for: {file_path}\")\n",
        "        return True\n",
        "    else:\n",
        "        logger.error(f\"Checksum mismatch for: {file_path}\")\n",
        "        logger.error(f\"  Expected: {expected_checksum}\")\n",
        "        logger.error(f\"  Actual:   {calculated_checksum}\")\n",
        "        return False\n",
        "\n",
        "def atomic_write_json(data: Any, file_path: str):\n",
        "    \"\"\"Write JSON data to a file atomically.\"\"\"\n",
        "    # Ensure the directory exists\n",
        "    dir_name = os.path.dirname(file_path)\n",
        "    if dir_name: # Only create if not in the current directory\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "    # Use tempfile for secure temporary file creation\n",
        "    try:\n",
        "        # Create temp file in the same directory to ensure atomic rename works across filesystems\n",
        "        with tempfile.NamedTemporaryFile('w', encoding='utf-8', delete=False, dir=dir_name, suffix='.tmp') as tf:\n",
        "            temp_file_path = tf.name\n",
        "            json.dump(data, tf, indent=2)\n",
        "            # Ensure data is written to disk before renaming\n",
        "            tf.flush()\n",
        "            os.fsync(tf.fileno())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to write temporary JSON file in {dir_name}: {e}\")\n",
        "        # Clean up temp file if it exists and write failed\n",
        "        if 'temp_file_path' in locals() and os.path.exists(temp_file_path):\n",
        "            try:\n",
        "                os.remove(temp_file_path)\n",
        "            except OSError as rm_err:\n",
        "                logger.error(f\"Failed to remove temporary file {temp_file_path}: {rm_err}\")\n",
        "        raise # Re-raise the original exception\n",
        "\n",
        "    try:\n",
        "        # Atomic rename/replace operation\n",
        "        os.replace(temp_file_path, file_path)\n",
        "        logger.debug(f\"Atomically wrote JSON to: {file_path}\")\n",
        "    except OSError as e:\n",
        "        logger.error(f\"Failed to replace {file_path} with {temp_file_path}: {e}\")\n",
        "        # Clean up temp file if rename failed\n",
        "        if os.path.exists(temp_file_path):\n",
        "            try:\n",
        "                os.remove(temp_file_path)\n",
        "            except OSError as rm_err:\n",
        "                logger.error(f\"Failed to remove temporary file {temp_file_path}: {rm_err}\")\n",
        "        raise # Re-raise the original exception\n",
        "\n",
        "def load_json_safe(file_path: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Load JSON data from a file safely, handling errors.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.debug(f\"JSON file not found: {file_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except json.JSONDecodeError as e:\n",
        "        logger.error(f\"Error decoding JSON from {file_path}: {e}\")\n",
        "        # Attempt to move corrupted file\n",
        "        try:\n",
        "            corrupt_path = f\"{file_path}.corrupt_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
        "            shutil.move(file_path, corrupt_path)\n",
        "            logger.warning(f\"Moved potentially corrupted JSON file to: {corrupt_path}\")\n",
        "        except Exception as move_e:\n",
        "            logger.error(f\"Could not move corrupted JSON file {file_path}: {move_e}\")\n",
        "        return None\n",
        "    except OSError as e:\n",
        "        logger.error(f\"Error reading JSON file {file_path}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error loading JSON file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_file_mime_type(file_path: str) -> str:\n",
        "    \"\"\"Get MIME type using python-magic or fallback to extension.\"\"\"\n",
        "    if MAGIC_AVAILABLE:\n",
        "        try:\n",
        "            # Ensure the magic object is created fresh or handled thread-safely if needed\n",
        "            # Creating it each time is safer for multiprocessing/threading unless explicitly managed\n",
        "            mime_detector = magic.Magic(mime=True)\n",
        "            return mime_detector.from_file(file_path)\n",
        "        except magic.MagicException as e:\n",
        "            logger.warning(f\"python-magic failed for {file_path}: {e}. Falling back to extension.\")\n",
        "            return get_mime_from_extension(file_path)\n",
        "        except FileNotFoundError:\n",
        "             logger.error(f\"File not found by python-magic: {file_path}\")\n",
        "             return get_mime_from_extension(file_path)\n",
        "        except OSError as e:\n",
        "             # Catch OS errors like permission denied during magic processing\n",
        "             logger.error(f\"OS error getting MIME type for {file_path}: {e}\")\n",
        "             return get_mime_from_extension(file_path)\n",
        "        except Exception as e:\n",
        "            # Catch other potential errors\n",
        "            logger.error(f\"Unexpected error getting MIME type for {file_path}: {e}\")\n",
        "            return get_mime_from_extension(file_path)\n",
        "    else:\n",
        "        logger.debug(\"python-magic not available, using extension-based MIME type.\")\n",
        "        return get_mime_from_extension(file_path)\n",
        "\n",
        "def get_mime_from_extension(file_path: str) -> str:\n",
        "    \"\"\"Guess MIME type from file extension (basic fallback).\"\"\"\n",
        "    extension = os.path.splitext(file_path)[1].lower()\n",
        "    # Simplified common MIME types\n",
        "    mime_types = {\n",
        "        '.txt': 'text/plain', '.log': 'text/plain', '.md': 'text/markdown',\n",
        "        '.html': 'text/html', '.htm': 'text/html', '.css': 'text/css', '.js': 'application/javascript',\n",
        "        '.json': 'application/json', '.xml': 'application/xml', '.csv': 'text/csv',\n",
        "        '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.png': 'image/png', '.gif': 'image/gif',\n",
        "        '.svg': 'image/svg+xml', '.webp': 'image/webp', '.bmp': 'image/bmp', '.tiff': 'image/tiff', '.ico': 'image/vnd.microsoft.icon',\n",
        "        '.mp3': 'audio/mpeg', '.wav': 'audio/wav', '.ogg': 'audio/ogg', '.m4a': 'audio/mp4', '.flac': 'audio/flac',\n",
        "        '.mp4': 'video/mp4', '.avi': 'video/x-msvideo', '.mov': 'video/quicktime', '.wmv': 'video/x-ms-wmv', '.mkv': 'video/x-matroska', '.webm': 'video/webm',\n",
        "        '.pdf': 'application/pdf',\n",
        "        '.doc': 'application/msword', '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
        "        '.xls': 'application/vnd.ms-excel', '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
        "        '.ppt': 'application/vnd.ms-powerpoint', '.pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',\n",
        "        '.odt': 'application/vnd.oasis.opendocument.text', '.ods': 'application/vnd.oasis.opendocument.spreadsheet', '.odp': 'application/vnd.oasis.opendocument.presentation',\n",
        "        '.zip': 'application/zip', '.rar': 'application/vnd.rar', '.tar': 'application/x-tar',\n",
        "        '.gz': 'application/gzip', '.bz2': 'application/x-bzip2', '.7z': 'application/x-7z-compressed',\n",
        "        '.exe': 'application/x-msdownload', '.dll': 'application/x-msdownload', '.so': 'application/octet-stream',\n",
        "        '.apk': 'application/vnd.android.package-archive', '.ipa': 'application/octet-stream', # iOS app\n",
        "        '.iso': 'application/x-iso9660-image', '.img': 'application/octet-stream', # Disk images\n",
        "        '.db': 'application/vnd.sqlite3', '.sqlite': 'application/vnd.sqlite3', '.sqlite3': 'application/vnd.sqlite3', '.sql': 'application/sql',\n",
        "        '.vcf': 'text/vcard', '.ics': 'text/calendar',\n",
        "        '.ttf': 'font/ttf', '.otf': 'font/otf', '.woff': 'font/woff', '.woff2': 'font/woff2',\n",
        "        # Add more as needed\n",
        "    }\n",
        "    return mime_types.get(extension, 'application/octet-stream') # Default binary stream\n",
        "\n",
        "def format_bytes(size: int) -> str:\n",
        "    \"\"\"Format bytes into a human-readable string (KB, MB, GB).\"\"\"\n",
        "    if size < 1024:\n",
        "        return f\"{size} B\"\n",
        "    elif size < 1024**2:\n",
        "        return f\"{size/1024:.2f} KB\"\n",
        "    elif size < 1024**3:\n",
        "        return f\"{size/1024**2:.2f} MB\"\n",
        "    else:\n",
        "        return f\"{size/1024**3:.2f} GB\"\n",
        "\n",
        "def sanitize_filename(filename: str) -> str:\n",
        "    \"\"\"Sanitize a string to be used as a valid filename.\"\"\"\n",
        "    # Remove invalid characters for most filesystems\n",
        "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
        "    # Replace multiple underscores with single underscore\n",
        "    sanitized = re.sub(r'_+', '_', sanitized)\n",
        "    # Remove leading/trailing underscores/spaces\n",
        "    sanitized = sanitized.strip('_. ')\n",
        "    # Limit length (e.g., 200 chars) to avoid issues\n",
        "    max_len = 200\n",
        "    if len(sanitized) > max_len:\n",
        "        # Keep extension if present\n",
        "        base, ext = os.path.splitext(sanitized)\n",
        "        sanitized = base[:max_len - len(ext)] + ext\n",
        "    # Handle empty filenames after sanitization\n",
        "    if not sanitized:\n",
        "        return f\"sanitized_{uuid.uuid4().hex[:8]}\"\n",
        "    return sanitized\n",
        "\n",
        "def parse_iso_datetime(date_str: Optional[str]) -> Optional[datetime]:\n",
        "    \"\"\"Safely parse an ISO format datetime string.\"\"\"\n",
        "    if not date_str:\n",
        "        return None\n",
        "    try:\n",
        "        # Handle potential timezone offsets (Z, +HH:MM, -HH:MM)\n",
        "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "    except (ValueError, TypeError):\n",
        "        logger.warning(f\"Could not parse datetime string: {date_str}\")\n",
        "        return None\n",
        "\n",
        "# --- Signal Handler ---\n",
        "def signal_handler(signum, frame):\n",
        "    \"\"\"Handle termination signals gracefully.\"\"\"\n",
        "    signal_name = signal.Signals(signum).name\n",
        "    logger.warning(f\"Received signal: {signal_name}. Initiating graceful shutdown...\")\n",
        "    stop_event.set() # Signal threads/processes to stop\n",
        "\n",
        "# --- Analysis Supervisor ---\n",
        "\n",
        "class AnalysisSupervisor:\n",
        "    \"\"\"Monitors and manages the analysis process state.\"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.start_time = time.time()\n",
        "        self.state = {\n",
        "            \"scriptVersion\": SCRIPT_VERSION,\n",
        "            \"pid\": os.getpid(),\n",
        "            \"status\": \"initializing\",\n",
        "            \"phase\": \"startup\",\n",
        "            \"progressPercent\": 0.0,\n",
        "            \"processedFiles\": 0,\n",
        "            \"totalFilesToProcess\": 0,\n",
        "            \"processedMessages\": 0, # Specific to message phase\n",
        "            \"totalMessagesToProcess\": 0, # Specific to message phase\n",
        "            \"startTime\": self.start_time,\n",
        "            \"lastUpdateTime\": self.start_time,\n",
        "            \"estimatedEndTime\": None,\n",
        "            \"currentTask\": \"Initializing\",\n",
        "            \"errors\": [],\n",
        "            \"warnings\": [],\n",
        "            \"stats\": defaultdict(int),\n",
        "            \"config\": {k: v for k, v in asdict(config).items() if k not in ['db_path', 'checkpoint_path', 'state_file_path', 'pid_file_path', 'visualization_data_path']} # Store config for reference, exclude sensitive paths\n",
        "        }\n",
        "        self.lock = threading.Lock() # Use threading lock for thread safety\n",
        "        self._write_pid_file()\n",
        "        self.save_state() # Initial state save\n",
        "\n",
        "    def _write_pid_file(self):\n",
        "        \"\"\"Write the current process ID to the PID file.\"\"\"\n",
        "        try:\n",
        "            with open(self.config.pid_file_path, 'w') as f:\n",
        "                f.write(str(os.getpid()))\n",
        "            logger.info(f\"PID file created at: {self.config.pid_file_path} with PID {os.getpid()}\")\n",
        "        except OSError as e:\n",
        "            logger.error(f\"Failed to write PID file {self.config.pid_file_path}: {e}\")\n",
        "\n",
        "    def _remove_pid_file(self):\n",
        "        \"\"\"Remove the PID file.\"\"\"\n",
        "        try:\n",
        "            # Verify PID before removing, in case the process died and another started\n",
        "            if os.path.exists(self.config.pid_file_path):\n",
        "                 with open(self.config.pid_file_path, 'r') as f:\n",
        "                     pid_in_file = f.read().strip()\n",
        "                     if pid_in_file == str(os.getpid()):\n",
        "                         os.remove(self.config.pid_file_path)\n",
        "                         logger.info(\"PID file removed.\")\n",
        "                     else:\n",
        "                         logger.warning(f\"PID file {self.config.pid_file_path} belongs to another process ({pid_in_file}), not removing.\")\n",
        "            else:\n",
        "                logger.info(\"PID file not found, nothing to remove.\")\n",
        "\n",
        "        except OSError as e:\n",
        "            logger.error(f\"Failed to remove PID file {self.config.pid_file_path}: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error removing PID file: {e}\")\n",
        "\n",
        "\n",
        "    def save_state(self):\n",
        "        \"\"\"Save the current state to the state file.\"\"\"\n",
        "        with self.lock:\n",
        "            current_time = time.time()\n",
        "            self.state[\"lastUpdateTime\"] = current_time\n",
        "            # Format times for readability in JSON\n",
        "            current_state = self.state.copy()\n",
        "            current_state[\"startTimeFormatted\"] = datetime.fromtimestamp(self.state[\"startTime\"]).isoformat()\n",
        "            current_state[\"lastUpdateTimeFormatted\"] = datetime.fromtimestamp(current_time).isoformat()\n",
        "            if self.state[\"estimatedEndTime\"]:\n",
        "                 current_state[\"estimatedEndTimeFormatted\"] = datetime.fromtimestamp(self.state[\"estimatedEndTime\"]).isoformat()\n",
        "            else:\n",
        "                 current_state[\"estimatedEndTimeFormatted\"] = None # Ensure it's null if not set\n",
        "\n",
        "            # Convert defaultdict to regular dict for JSON serialization\n",
        "            current_state[\"stats\"] = dict(self.state[\"stats\"])\n",
        "\n",
        "            try:\n",
        "                atomic_write_json(current_state, self.config.state_file_path)\n",
        "            except Exception as e:\n",
        "                # Use print as logger might be involved in the failure\n",
        "                print(f\"CRITICAL: Failed to save state file: {e}\", file=sys.stderr)\n",
        "                logger.critical(f\"Failed to save state file: {e}\", exc_info=True)\n",
        "\n",
        "    def update_status(self, status: str, task: Optional[str] = None):\n",
        "        \"\"\"Update the overall status and current task.\"\"\"\n",
        "        with self.lock:\n",
        "            self.state[\"status\"] = status\n",
        "            if task:\n",
        "                self.state[\"currentTask\"] = task\n",
        "                logger.info(f\"Status: {status} - Task: {task}\")\n",
        "            else:\n",
        "                # If no task provided, keep the last task or set a default\n",
        "                self.state[\"currentTask\"] = self.state.get(\"currentTask\", \"N/A\")\n",
        "                logger.info(f\"Status: {status} (Task: {self.state['currentTask']})\")\n",
        "        self.save_state()\n",
        "\n",
        "    def update_phase(self, phase: str):\n",
        "        \"\"\"Update the current analysis phase.\"\"\"\n",
        "        with self.lock:\n",
        "            self.state[\"phase\"] = phase\n",
        "            self.state[\"currentTask\"] = f\"Starting {phase}\" # Reset task for new phase\n",
        "            logger.info(f\"Entering phase: {phase}\")\n",
        "        self.save_state()\n",
        "\n",
        "    def set_file_totals(self, total_files: int):\n",
        "        \"\"\"Set the total number of files to be processed.\"\"\"\n",
        "        with self.lock:\n",
        "            self.state[\"totalFilesToProcess\"] = total_files\n",
        "            # Reset progress if totals are reset\n",
        "            self.state[\"processedFiles\"] = 0\n",
        "            self.state[\"progressPercent\"] = 0.0\n",
        "            self.state[\"estimatedEndTime\"] = None\n",
        "        self.save_state()\n",
        "\n",
        "    def update_file_progress(self, increment: int = 1):\n",
        "        \"\"\"Update the progress of file processing.\"\"\"\n",
        "        if increment <= 0: return\n",
        "\n",
        "        with self.lock:\n",
        "            self.state[\"processedFiles\"] += increment\n",
        "            total = self.state[\"totalFilesToProcess\"]\n",
        "            processed = self.state[\"processedFiles\"]\n",
        "\n",
        "            # Ensure processed doesn't exceed total\n",
        "            processed = min(processed, total)\n",
        "            self.state[\"processedFiles\"] = processed\n",
        "\n",
        "            if total > 0:\n",
        "                self.state[\"progressPercent\"] = round((processed / total) * 100, 2)\n",
        "                # Estimate remaining time\n",
        "                elapsed_time = time.time() - self.start_time\n",
        "                if processed > 10 and elapsed_time > 5: # Avoid division by zero and unstable early estimates\n",
        "                    try:\n",
        "                        time_per_file = elapsed_time / processed\n",
        "                        remaining_files = total - processed\n",
        "                        if remaining_files > 0:\n",
        "                            remaining_time = remaining_files * time_per_file\n",
        "                            self.state[\"estimatedEndTime\"] = time.time() + remaining_time\n",
        "                        else:\n",
        "                            # If processing is complete, clear ETA\n",
        "                            self.state[\"estimatedEndTime\"] = None\n",
        "                    except ZeroDivisionError:\n",
        "                        self.state[\"estimatedEndTime\"] = None # Avoid error if processed becomes 0 somehow\n",
        "                else:\n",
        "                    self.state[\"estimatedEndTime\"] = None # Not enough data for reliable ETA\n",
        "            else:\n",
        "                self.state[\"progressPercent\"] = 100.0 if processed > 0 else 0.0 # Handle case where total is 0\n",
        "                self.state[\"estimatedEndTime\"] = None\n",
        "\n",
        "            # Log progress periodically\n",
        "            log_interval = max(100, self.config.batch_size) # Log every batch or 100 files\n",
        "            if processed % log_interval == 0 or processed == total:\n",
        "                 est_time_str = \"\"\n",
        "                 if self.state[\"estimatedEndTime\"]:\n",
        "                     try:\n",
        "                         eta_dt = datetime.fromtimestamp(self.state['estimatedEndTime'])\n",
        "                         est_time_str = f\", ETA: {eta_dt.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "                     except TypeError: # Handle potential None value if calculation failed\n",
        "                         est_time_str = \", ETA: Calculating...\"\n",
        "                 logger.info(f\"File Progress: {processed}/{total} ({self.state['progressPercent']}%) {est_time_str}\")\n",
        "\n",
        "        # Save state less frequently for performance during high-volume updates\n",
        "        # Save every N updates or if progress changed significantly\n",
        "        if processed % 50 == 0 or processed == total:\n",
        "             self.save_state()\n",
        "\n",
        "\n",
        "    def set_message_totals(self, total_messages: int):\n",
        "        \"\"\"Set the total number of messages to be processed.\"\"\"\n",
        "        with self.lock:\n",
        "            self.state[\"totalMessagesToProcess\"] = total_messages\n",
        "            # Reset progress for message phase\n",
        "            self.state[\"processedMessages\"] = 0\n",
        "        self.save_state()\n",
        "\n",
        "    def update_message_progress(self, increment: int = 1):\n",
        "        \"\"\"Update the progress of message processing.\"\"\"\n",
        "        if increment <= 0: return\n",
        "\n",
        "        with self.lock:\n",
        "            self.state[\"processedMessages\"] += increment\n",
        "            total = self.state[\"totalMessagesToProcess\"]\n",
        "            processed = self.state[\"processedMessages\"]\n",
        "\n",
        "            # Ensure processed doesn't exceed total\n",
        "            processed = min(processed, total)\n",
        "            self.state[\"processedMessages\"] = processed\n",
        "\n",
        "            # Log message progress if needed, similar to file progress\n",
        "            log_interval = 1000\n",
        "            if processed % log_interval == 0 or processed == total:\n",
        "                 progress_percent = round((processed / total) * 100, 1) if total > 0 else 100.0\n",
        "                 logger.info(f\"Message Progress: {processed}/{total} ({progress_percent}%)\")\n",
        "\n",
        "        # Save state periodically for message progress\n",
        "        if processed % 200 == 0 or processed == total:\n",
        "            self.save_state()\n",
        "\n",
        "    def log_error(self, message: str, detail: Optional[str] = None):\n",
        "        \"\"\"Log an error and add it to the state.\"\"\"\n",
        "        log_message = f\"{message} - Detail: {detail}\" if detail else message\n",
        "        logger.error(log_message) # Log first\n",
        "        with self.lock:\n",
        "            error_entry = {\n",
        "                \"timestamp\": time.time(),\n",
        "                \"timestampFormatted\": datetime.now().isoformat(),\n",
        "                \"message\": message,\n",
        "                \"detail\": detail\n",
        "            }\n",
        "            self.state[\"errors\"].append(error_entry)\n",
        "            # Limit error list size in state file for performance\n",
        "            max_errors_in_state = 50\n",
        "            if len(self.state[\"errors\"]) > max_errors_in_state:\n",
        "                self.state[\"errors\"] = self.state[\"errors\"][-max_errors_in_state:]\n",
        "        self.save_state() # Save state after logging error\n",
        "\n",
        "    def log_warning(self, message: str, detail: Optional[str] = None):\n",
        "        \"\"\"Log a warning and add it to the state.\"\"\"\n",
        "        log_message = f\"{message} - Detail: {detail}\" if detail else message\n",
        "        logger.warning(log_message) # Log first\n",
        "        with self.lock:\n",
        "            warning_entry = {\n",
        "                \"timestamp\": time.time(),\n",
        "                \"timestampFormatted\": datetime.now().isoformat(),\n",
        "                \"message\": message,\n",
        "                \"detail\": detail\n",
        "            }\n",
        "            self.state[\"warnings\"].append(warning_entry)\n",
        "            # Limit warning list size in state file\n",
        "            max_warnings_in_state = 50\n",
        "            if len(self.state[\"warnings\"]) > max_warnings_in_state:\n",
        "                self.state[\"warnings\"] = self.state[\"warnings\"][-max_warnings_in_state:]\n",
        "        self.save_state() # Save state after logging warning\n",
        "\n",
        "    def update_stat(self, key: str, increment: int = 1):\n",
        "        \"\"\"Update a specific statistic counter.\"\"\"\n",
        "        with self.lock:\n",
        "            self.state[\"stats\"][key] = self.state[\"stats\"].get(key, 0) + increment\n",
        "        # Don't save state on every stat update for performance\n",
        "        # It will be saved on the next status/progress update or explicit save call\n",
        "\n",
        "    def get_state(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return a copy of the current state.\"\"\"\n",
        "        with self.lock:\n",
        "            # Ensure defaultdict is converted to dict for the returned copy\n",
        "            state_copy = self.state.copy()\n",
        "            state_copy[\"stats\"] = dict(self.state[\"stats\"])\n",
        "            return state_copy\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Perform cleanup actions for the supervisor.\"\"\"\n",
        "        logger.info(\"Closing analysis supervisor...\")\n",
        "        # Final state save before removing PID\n",
        "        self.update_status(\"finished\", \"Analysis complete\")\n",
        "        self._remove_pid_file()\n",
        "        logger.info(\"Analysis supervisor closed.\")\n",
        "\n",
        "# --- Database Manager ---\n",
        "\n",
        "class DatabaseManager:\n",
        "    \"\"\"Manages interactions with the SQLite database.\"\"\"\n",
        "    def __init__(self, db_path: str):\n",
        "        self.db_path = db_path\n",
        "        self.conn = None\n",
        "        self._connect()\n",
        "        self._setup_database()\n",
        "\n",
        "    def _connect(self):\n",
        "        \"\"\"Establish connection to the SQLite database.\"\"\"\n",
        "        try:\n",
        "            # Timeout increased for potentially long operations\n",
        "            # Check for thread safety: SQLite default build is threadsafe (mode 3)\n",
        "            # Ensure check_same_thread=False is NOT used unless external locking is implemented\n",
        "            self.conn = sqlite3.connect(self.db_path, timeout=30.0, check_same_thread=True)\n",
        "            self.conn.row_factory = sqlite3.Row # Access columns by name\n",
        "            # Enable Write-Ahead Logging for better concurrency\n",
        "            self.conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
        "            # Set busy timeout to handle concurrent writes gracefully\n",
        "            self.conn.execute(\"PRAGMA busy_timeout = 5000;\") # 5 seconds\n",
        "            # Enforce foreign key constraints\n",
        "            self.conn.execute(\"PRAGMA foreign_keys=ON;\")\n",
        "            logger.info(f\"Connected to database: {self.db_path}\")\n",
        "        except sqlite3.Error as e:\n",
        "            logger.critical(f\"Database connection failed: {e}\")\n",
        "            raise # Critical error, cannot proceed\n",
        "\n",
        "    def _setup_database(self):\n",
        "        \"\"\"Create database tables if they don't exist.\"\"\"\n",
        "        if not self.conn:\n",
        "            logger.error(\"Database not connected, cannot set up tables.\")\n",
        "            return\n",
        "\n",
        "        setup_statements = [\n",
        "            # Files table (Consolidated from file_analyzer)\n",
        "            '''\n",
        "            CREATE TABLE IF NOT EXISTS files (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                path TEXT NOT NULL,\n",
        "                filename TEXT NOT NULL,\n",
        "                extension TEXT,\n",
        "                filesize INTEGER DEFAULT 0,\n",
        "                mimetype TEXT DEFAULT 'application/octet-stream',\n",
        "                md5hash TEXT,\n",
        "                fuzzyhash TEXT,\n",
        "                creation_date TEXT,      -- Store as ISO 8601 string\n",
        "                modification_date TEXT,  -- Store as ISO 8601 string\n",
        "                category TEXT DEFAULT 'Other',\n",
        "                value_score REAL DEFAULT 0.0,\n",
        "                is_duplicate INTEGER DEFAULT 0,\n",
        "                original_id INTEGER,     -- References files(id)\n",
        "                last_analyzed TEXT,      -- Store as ISO 8601 string\n",
        "                error TEXT,\n",
        "                UNIQUE(path, filename),\n",
        "                FOREIGN KEY (original_id) REFERENCES files (id) ON DELETE SET NULL\n",
        "            )\n",
        "            ''',\n",
        "            # Patterns table (Consolidated from file_analyzer)\n",
        "            '''\n",
        "            CREATE TABLE IF NOT EXISTS patterns (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                file_id INTEGER NOT NULL,\n",
        "                pattern_type TEXT NOT NULL,\n",
        "                pattern_value TEXT NOT NULL,\n",
        "                confidence REAL DEFAULT 1.0,\n",
        "                FOREIGN KEY (file_id) REFERENCES files (id) ON DELETE CASCADE\n",
        "            )\n",
        "            ''',\n",
        "            # Index for faster pattern lookups\n",
        "            'CREATE INDEX IF NOT EXISTS idx_patterns_file_id ON patterns (file_id)',\n",
        "            'CREATE INDEX IF NOT EXISTS idx_patterns_type_value ON patterns (pattern_type, pattern_value)',\n",
        "            # Index for faster file lookups by hash or path\n",
        "            'CREATE INDEX IF NOT EXISTS idx_files_md5hash ON files (md5hash)',\n",
        "            'CREATE INDEX IF NOT EXISTS idx_files_path_filename ON files (path, filename)',\n",
        "            'CREATE INDEX IF NOT EXISTS idx_files_category ON files (category)',\n",
        "            'CREATE INDEX IF NOT EXISTS idx_files_modification_date ON files (modification_date)',\n",
        "            # Checkpoints table (Consolidated from file_analyzer) - Consider if needed with JSON checkpoint\n",
        "            # Removing Checkpoints table as JSON checkpoint is used for file analysis resume\n",
        "            # '''\n",
        "            # CREATE TABLE IF NOT EXISTS checkpoints (\n",
        "            #     id INTEGER PRIMARY KEY,\n",
        "            #     checkpoint_time TEXT NOT NULL,\n",
        "            #     processed_files INTEGER NOT NULL,\n",
        "            #     total_files INTEGER NOT NULL,\n",
        "            #     sample_rate REAL,\n",
        "            #     status TEXT,\n",
        "            #     details TEXT -- Store JSON details like last processed path etc.\n",
        "            # )\n",
        "            # '''\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            cursor = self.conn.cursor()\n",
        "            for statement in setup_statements:\n",
        "                cursor.execute(statement)\n",
        "            self.conn.commit()\n",
        "            logger.info(\"Database schema setup/verified.\")\n",
        "        except sqlite3.Error as e:\n",
        "            logger.critical(f\"Database setup failed: {e}\")\n",
        "            if self.conn:\n",
        "                self.conn.rollback() # Rollback any partial changes\n",
        "            raise\n",
        "\n",
        "    @contextmanager\n",
        "    def get_cursor(self):\n",
        "        \"\"\"Provide a database cursor within a context manager for safe transactions.\"\"\"\n",
        "        if not self.conn:\n",
        "            logger.warning(\"Database connection lost. Attempting to reconnect...\")\n",
        "            self._connect() # Try to reconnect\n",
        "            if not self.conn:\n",
        "                 logger.error(\"Database reconnection failed.\")\n",
        "                 raise sqlite3.Error(\"Database connection is not available.\")\n",
        "\n",
        "        cursor = None\n",
        "        try:\n",
        "            # Start transaction implicitly\n",
        "            cursor = self.conn.cursor()\n",
        "            yield cursor\n",
        "            self.conn.commit() # Commit successful transactions\n",
        "        except sqlite3.Error as e:\n",
        "            logger.error(f\"Database transaction failed: {e}\")\n",
        "            if self.conn:\n",
        "                try:\n",
        "                    self.conn.rollback() # Rollback on error\n",
        "                    logger.info(\"Database transaction rolled back.\")\n",
        "                except sqlite3.Error as rb_err:\n",
        "                    logger.error(f\"Database rollback failed: {rb_err}\")\n",
        "            raise # Re-raise the original exception\n",
        "        # Cursor is automatically managed by the connection\n",
        "\n",
        "    def execute_query(self, query: str, params: tuple = ()) -> List[sqlite3.Row]:\n",
        "        \"\"\"Execute a SELECT query with retry logic for transient errors.\"\"\"\n",
        "        last_error = None\n",
        "        for attempt in range(MAX_RETRIES + 1): # Try MAX_RETRIES times + 1 initial try\n",
        "            if stop_event.is_set():\n",
        "                 logger.warning(\"Stop event set, aborting DB query.\")\n",
        "                 raise InterruptedError(\"Database query aborted due to stop signal.\")\n",
        "            try:\n",
        "                # No need for explicit transaction management here, SELECTs are usually safe\n",
        "                cursor = self.conn.cursor()\n",
        "                cursor.execute(query, params)\n",
        "                return cursor.fetchall()\n",
        "            except (sqlite3.OperationalError, sqlite3.DatabaseError) as e:\n",
        "                last_error = e\n",
        "                # Check for specific transient errors like \"database is locked\" or \"busy\"\n",
        "                err_msg = str(e).lower()\n",
        "                if \"database is locked\" in err_msg or \"busy\" in err_msg:\n",
        "                    if attempt < MAX_RETRIES:\n",
        "                        delay = RETRY_DELAY * (attempt + 1) * (random.random() * 0.5 + 0.75) # Add jitter\n",
        "                        logger.warning(f\"DB Query lock/busy (Attempt {attempt+1}/{MAX_RETRIES}): {e}. Retrying in {delay:.2f}s...\")\n",
        "                        time.sleep(delay)\n",
        "                        continue # Retry the operation\n",
        "                    else:\n",
        "                        logger.error(f\"DB Query failed due to lock/busy after {MAX_RETRIES} retries.\")\n",
        "                        break # Max retries exceeded\n",
        "                else:\n",
        "                    # For other operational/database errors, log and break (don't retry usually)\n",
        "                    logger.error(f\"DB Query failed with non-transient error: {e}\")\n",
        "                    break\n",
        "            except sqlite3.Error as e: # Catch other SQLite errors\n",
        "                last_error = e\n",
        "                logger.error(f\"DB Query failed with SQLite error: {e}\")\n",
        "                break # Don't retry other SQLite errors\n",
        "            except Exception as e: # Catch unexpected errors\n",
        "                last_error = e\n",
        "                logger.error(f\"Unexpected error during DB query: {e}\", exc_info=True)\n",
        "                break\n",
        "\n",
        "        logger.error(f\"DB Query failed permanently. Last error: {last_error}. Query: {query[:200]}...\")\n",
        "        raise last_error if last_error else sqlite3.Error(\"DB Query failed for unknown reasons.\")\n",
        "\n",
        "\n",
        "    def execute_write(self, query: str, params: tuple = ()) -> Optional[int]:\n",
        "        \"\"\"Execute an INSERT, UPDATE, or DELETE query within a transaction with retry logic.\"\"\"\n",
        "        last_error = None\n",
        "        for attempt in range(MAX_RETRIES + 1):\n",
        "            if stop_event.is_set():\n",
        "                 logger.warning(\"Stop event set, aborting DB write.\")\n",
        "                 raise InterruptedError(\"Database write aborted due to stop signal.\")\n",
        "            try:\n",
        "                with self.get_cursor() as cursor:\n",
        "                    cursor.execute(query, params)\n",
        "                    return cursor.lastrowid # Return ID for INSERTs or None\n",
        "            except (sqlite3.OperationalError, sqlite3.DatabaseError) as e:\n",
        "                last_error = e\n",
        "                err_msg = str(e).lower()\n",
        "                if \"database is locked\" in err_msg or \"busy\" in err_msg:\n",
        "                    if attempt < MAX_RETRIES:\n",
        "                        delay = RETRY_DELAY * (attempt + 1) * (random.random() * 0.5 + 0.75)\n",
        "                        logger.warning(f\"DB Write lock/busy (Attempt {attempt+1}/{MAX_RETRIES}): {e}. Retrying in {delay:.2f}s...\")\n",
        "                        time.sleep(delay)\n",
        "                        continue\n",
        "                    else:\n",
        "                        logger.error(f\"DB Write failed due to lock/busy after {MAX_RETRIES} retries.\")\n",
        "                        break\n",
        "                else:\n",
        "                    logger.error(f\"DB Write failed with non-transient error: {e}\")\n",
        "                    break\n",
        "            except sqlite3.Error as e:\n",
        "                last_error = e\n",
        "                logger.error(f\"DB Write failed with SQLite error: {e}\")\n",
        "                # Specific handling for constraint violations if needed\n",
        "                if \"unique constraint\" in str(e).lower():\n",
        "                    logger.warning(f\"Unique constraint violation during write: {e}. Query: {query[:100]}...\")\n",
        "                    # Decide whether to raise or handle (e.g., if using INSERT OR IGNORE)\n",
        "                break # Usually don't retry integrity errors etc.\n",
        "            except Exception as e:\n",
        "                last_error = e\n",
        "                logger.error(f\"Unexpected error during DB write: {e}\", exc_info=True)\n",
        "                break\n",
        "\n",
        "        logger.error(f\"DB Write failed permanently. Last error: {last_error}. Query: {query[:200]}...\")\n",
        "        raise last_error if last_error else sqlite3.Error(\"DB Write failed for unknown reasons.\")\n",
        "\n",
        "\n",
        "    def execute_many(self, query: str, params_list: List[tuple]) -> int:\n",
        "        \"\"\"Execute a query with multiple parameter sets (executemany) within a transaction with retry.\"\"\"\n",
        "        if not params_list:\n",
        "            return 0\n",
        "        last_error = None\n",
        "        affected_rows = 0\n",
        "        for attempt in range(MAX_RETRIES + 1):\n",
        "            if stop_event.is_set():\n",
        "                 logger.warning(\"Stop event set, aborting DB executemany.\")\n",
        "                 raise InterruptedError(\"Database executemany aborted due to stop signal.\")\n",
        "            try:\n",
        "                with self.get_cursor() as cursor:\n",
        "                    cursor.executemany(query, params_list)\n",
        "                    affected_rows = cursor.rowcount # Return number of affected rows\n",
        "                    return affected_rows # Success\n",
        "            except (sqlite3.OperationalError, sqlite3.DatabaseError) as e:\n",
        "                last_error = e\n",
        "                err_msg = str(e).lower()\n",
        "                if \"database is locked\" in err_msg or \"busy\" in err_msg:\n",
        "                     if attempt < MAX_RETRIES:\n",
        "                         delay = RETRY_DELAY * (attempt + 1) * (random.random() * 0.5 + 0.75)\n",
        "                         logger.warning(f\"DB ExecuteMany lock/busy (Attempt {attempt+1}/{MAX_RETRIES}): {e}. Retrying in {delay:.2f}s...\")\n",
        "                         time.sleep(delay)\n",
        "                         continue\n",
        "                     else:\n",
        "                         logger.error(f\"DB ExecuteMany failed due to lock/busy after {MAX_RETRIES} retries.\")\n",
        "                         break\n",
        "                else:\n",
        "                    logger.error(f\"DB ExecuteMany failed with non-transient error: {e}\")\n",
        "                    break\n",
        "            except sqlite3.Error as e:\n",
        "                last_error = e\n",
        "                logger.error(f\"DB ExecuteMany failed with SQLite error: {e}\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                last_error = e\n",
        "                logger.error(f\"Unexpected error during DB executemany: {e}\", exc_info=True)\n",
        "                break\n",
        "\n",
        "        logger.error(f\"DB ExecuteMany failed permanently. Last error: {last_error}. Query: {query[:200]}...\")\n",
        "        # Return 0 or raise error? Raising seems more appropriate.\n",
        "        raise last_error if last_error else sqlite3.Error(\"DB ExecuteMany failed for unknown reasons.\")\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the database connection.\"\"\"\n",
        "        if self.conn:\n",
        "            try:\n",
        "                # Optimize database before closing - may take time on large DBs\n",
        "                logger.info(\"Optimizing database before closing (PRAGMA optimize)...\")\n",
        "                self.conn.execute(\"PRAGMA optimize;\")\n",
        "                # Ensure WAL checkpoint is written\n",
        "                logger.info(\"Running WAL checkpoint...\")\n",
        "                self.conn.execute(\"PRAGMA wal_checkpoint(TRUNCATE);\")\n",
        "                self.conn.commit() # Commit any final changes\n",
        "                self.conn.close()\n",
        "                logger.info(\"Database connection closed.\")\n",
        "                self.conn = None\n",
        "            except sqlite3.Error as e:\n",
        "                logger.error(f\"Error during database closing procedures: {e}\")\n",
        "            except Exception as e:\n",
        "                 logger.error(f\"Unexpected error closing database: {e}\")\n",
        "\n",
        "\n",
        "# --- File Analyzer ---\n",
        "\n",
        "class FileAnalyzer:\n",
        "    \"\"\"Handles the analysis of individual files.\"\"\"\n",
        "    def __init__(self, config: Config, db_manager: DatabaseManager, supervisor: AnalysisSupervisor):\n",
        "        self.config = config\n",
        "        self.db_manager = db_manager\n",
        "        self.supervisor = supervisor\n",
        "        self.processed_files_in_run = set() # Track files processed in this specific run/resume\n",
        "\n",
        "    def analyze_file(self, file_path: str) -> Optional[FileInfo]:\n",
        "        \"\"\"Analyze a single file and return its information. Designed to run in a separate process.\"\"\"\n",
        "        logger.debug(f\"[Worker {os.getpid()}] Analyzing file: {file_path}\")\n",
        "        file_info = FileInfo(path=os.path.dirname(file_path), filename=os.path.basename(file_path))\n",
        "\n",
        "        try:\n",
        "            # Basic file stats\n",
        "            stat_info = os.stat(file_path)\n",
        "            file_info.filesize = stat_info.st_size\n",
        "            # Use ISO format for consistency\n",
        "            file_info.modification_date = datetime.fromtimestamp(stat_info.st_mtime).isoformat()\n",
        "            # Platform-specific creation time handling\n",
        "            try:\n",
        "                file_info.creation_date = datetime.fromtimestamp(stat_info.st_birthtime).isoformat() # macOS, some BSD\n",
        "            except AttributeError:\n",
        "                file_info.creation_date = datetime.fromtimestamp(stat_info.st_ctime).isoformat() # Windows, Linux (ctime might be change time)\n",
        "\n",
        "            file_info.extension = os.path.splitext(file_info.filename)[1].lower()\n",
        "\n",
        "            # MIME Type (re-initialize magic if needed per process/thread)\n",
        "            file_info.mimetype = get_file_mime_type(file_path) # Utility function handles magic availability\n",
        "\n",
        "            # Hashing\n",
        "            file_info.md5hash = self._calculate_md5(file_path)\n",
        "            # Fuzzy hash only for smaller files and if library available\n",
        "            # Check SSDEEP_AVAILABLE within the worker process context\n",
        "            if SSDEEP_AVAILABLE and 0 < file_info.filesize < 100 * 1024 * 1024: # 100 MB limit, skip 0-byte files\n",
        "                file_info.fuzzyhash = self._calculate_fuzzy_hash(file_path)\n",
        "\n",
        "            # Categorization\n",
        "            file_info.category = self._get_file_category(file_path, file_info.mimetype, file_info.filename)\n",
        "\n",
        "            # Filename Pattern Analysis\n",
        "            file_info.patterns = self._analyze_filename_patterns(file_info.filename)\n",
        "\n",
        "            # Value Score (placeholder for future logic)\n",
        "            file_info.value_score = self._calculate_value_score(file_info)\n",
        "\n",
        "            file_info.last_analyzed = datetime.now().isoformat()\n",
        "\n",
        "            logger.debug(f\"[Worker {os.getpid()}] Analysis complete for: {file_path}\")\n",
        "            return file_info\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"[Worker {os.getpid()}] File not found during analysis: {file_path}\")\n",
        "            file_info.error = \"File not found\"\n",
        "            # Return partial info with error, but don't log to supervisor from worker\n",
        "            return file_info\n",
        "        except OSError as e:\n",
        "            logger.error(f\"[Worker {os.getpid()}] OS error analyzing file {file_path}: {e}\")\n",
        "            file_info.error = f\"OS Error: {e}\"\n",
        "            return file_info\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[Worker {os.getpid()}] Unexpected error analyzing file {file_path}: {e}\", exc_info=True)\n",
        "            file_info.error = f\"Unexpected Error: {e}\"\n",
        "            return file_info\n",
        "\n",
        "    def _calculate_md5(self, file_path: str) -> str:\n",
        "        \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
        "        try:\n",
        "            hash_md5 = hashlib.md5()\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                # Read in larger chunks for potentially better performance\n",
        "                for chunk in iter(lambda: f.read(65536), b\"\"): # 64KB chunks\n",
        "                    hash_md5.update(chunk)\n",
        "            return hash_md5.hexdigest()\n",
        "        except OSError as e:\n",
        "            logger.error(f\"[Worker {os.getpid()}] Error calculating MD5 for {file_path}: {e}\")\n",
        "            return \"\"\n",
        "        except Exception as e:\n",
        "             logger.error(f\"[Worker {os.getpid()}] Unexpected error calculating MD5 for {file_path}: {e}\")\n",
        "             return \"\"\n",
        "\n",
        "\n",
        "    def _calculate_fuzzy_hash(self, file_path: str) -> str:\n",
        "        \"\"\"Calculate fuzzy hash using ssdeep.\"\"\"\n",
        "        # No need to check SSDEEP_AVAILABLE here, already checked in analyze_file\n",
        "        try:\n",
        "            # Ensure the file is readable before passing to ssdeep\n",
        "            # ssdeep.hash_from_file handles file reading internally\n",
        "            return ssdeep.hash_from_file(file_path)\n",
        "        except ssdeep.Error as e:\n",
        "            # Log less critical errors as warnings\n",
        "            logger.warning(f\"[Worker {os.getpid()}] ssdeep error calculating fuzzy hash for {file_path}: {e}\")\n",
        "            return \"\"\n",
        "        except OSError as e:\n",
        "            logger.error(f\"[Worker {os.getpid()}] OS error during fuzzy hash calculation for {file_path}: {e}\")\n",
        "            return \"\"\n",
        "        except Exception as e: # Catch potential ssdeep internal errors\n",
        "             logger.error(f\"[Worker {os.getpid()}] Unexpected error calculating fuzzy hash for {file_path}: {e}\")\n",
        "             return \"\"\n",
        "\n",
        "    def _get_file_category(self, file_path: str, mime_type: str, filename: str) -> str:\n",
        "        \"\"\"Determine file category based on MIME type, extension, and filename patterns.\"\"\"\n",
        "        mime_major = mime_type.split('/')[0] if '/' in mime_type else mime_type\n",
        "        mime_minor = mime_type.split('/')[1] if '/' in mime_type else \"\"\n",
        "        extension = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "        # Prioritize specific MIME types and extensions\n",
        "        if mime_major == \"image\": return \"Images\"\n",
        "        if mime_major == \"video\": return \"Videos\"\n",
        "        if mime_major == \"audio\": return \"Audio\"\n",
        "\n",
        "        if \"pdf\" in mime_minor: return \"Documents\"\n",
        "        if \"msword\" in mime_minor or \"wordprocessingml\" in mime_minor or extension in ['.doc', '.docx']: return \"Documents\"\n",
        "        if \"ms-excel\" in mime_minor or \"spreadsheetml\" in mime_minor or extension in ['.xls', '.xlsx']: return \"Spreadsheets\"\n",
        "        if \"ms-powerpoint\" in mime_minor or \"presentationml\" in mime_minor or extension in ['.ppt', '.pptx']: return \"Presentations\"\n",
        "        if \"rtf\" in mime_minor or extension == '.rtf': return \"Documents\"\n",
        "        if \"opendocument.text\" in mime_minor or extension == '.odt': return \"Documents\"\n",
        "        if \"opendocument.spreadsheet\" in mime_minor or extension == '.ods': return \"Spreadsheets\"\n",
        "        if \"opendocument.presentation\" in mime_minor or extension == '.odp': return \"Presentations\"\n",
        "        if mime_major == \"text\" and extension in ['.txt', '.md', '.log', '.csv', '.tsv']: return \"Text\" # More specific text category\n",
        "\n",
        "        # Message file detection (more robust)\n",
        "        # Check common message/contact extensions first\n",
        "        if extension in ['.vcf', '.vcard']: return \"Contacts\"\n",
        "        if extension in ['.ics', '.ical']: return \"Calendar\"\n",
        "        # Check filename patterns and MIME types\n",
        "        if any(p in filename.lower() for p in [\"sms\", \"mms\", \"message\", \"chat\", \"contact\", \"backup.contacts\"]) or \\\n",
        "           (filename.startswith('+') and extension == '.txt') or \\\n",
        "           \"message\" in mime_minor or \"chat\" in mime_minor:\n",
        "             # Avoid categorizing generic XML/JSON as messages unless filename suggests it\n",
        "             if not (mime_minor in ['xml', 'json'] and not any(p in filename.lower() for p in [\"sms\", \"mms\", \"message\", \"chat\"])):\n",
        "                 return \"Messages\"\n",
        "\n",
        "        if \"zip\" in mime_minor or \"rar\" in mime_minor or \"tar\" in mime_minor or \\\n",
        "           \"gzip\" in mime_minor or \"bzip2\" in mime_minor or \"7z\" in mime_minor or \\\n",
        "           extension in ['.zip', '.rar', '.tar', '.gz', '.bz2', '.7z']:\n",
        "            return \"Archives\"\n",
        "\n",
        "        if \"x-msdownload\" in mime_minor or \"executable\" in mime_minor or \\\n",
        "           extension in ['.exe', '.dll', '.app', '.bat', '.sh', '.msi', '.dmg', '.deb', '.rpm']:\n",
        "            return \"Applications\"\n",
        "        if \"android.package-archive\" in mime_minor or extension == '.apk': return \"Applications\" # APK\n",
        "        if extension == '.ipa': return \"Applications\" # iOS App\n",
        "\n",
        "        if \"database\" in mime_minor or \"sqlite\" in mime_minor or extension in ['.db', '.sqlite', '.sqlite3', '.sql']: return \"Databases\"\n",
        "        if mime_major == \"font\" or extension in ['.ttf', '.otf', '.woff', '.woff2']: return \"Fonts\"\n",
        "\n",
        "        # Fallback for generic types\n",
        "        if mime_major == \"text\": return \"Text\" # Generic text files\n",
        "        if mime_major == \"application\":\n",
        "             if mime_minor in ['xml', 'json', 'javascript', 'html', 'css']: return \"Code/Config\" # Common web/config formats\n",
        "             return \"Application Data\" # Other application-specific files\n",
        "\n",
        "        return \"Other\" # Default category\n",
        "\n",
        "    def _analyze_filename_patterns(self, filename: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Analyze filename for common patterns like dates, phone numbers, etc.\"\"\"\n",
        "        patterns = []\n",
        "        filename_lower = filename.lower()\n",
        "\n",
        "        # Phone number pattern (basic check in filename)\n",
        "        # Use the more robust extraction function (defined later globally)\n",
        "        # Note: This runs in worker, so global logger needs care if used inside extract_phone_numbers\n",
        "        # Pass None or a dummy logger if needed. Here, assuming extract_phone_numbers is safe.\n",
        "        phone_numbers = extract_phone_numbers(filename, None) # Pass None logger\n",
        "        for num in phone_numbers:\n",
        "             # Check length to avoid matching random long numbers\n",
        "             cleaned_num = re.sub(r'\\D', '', num)\n",
        "             if 6 < len(cleaned_num) < 16:\n",
        "                 patterns.append({'pattern_type': 'phone_number_fname', 'pattern_value': num, 'confidence': 0.7}) # Lower confidence from filename only\n",
        "\n",
        "        # Date patterns (YYYY-MM-DD,```python\n",
        "                    # Convert file list back to set\n",
        "                    c_dict['files'] = set(c_dict.get('files', []))\n",
        "                    # Handle potential missing fields gracefully\n",
        "                    contact = Contact(**{k: v for k, v in c_dict.items() if k in Contact.__annotations__})\n",
        "                    contacts.append(contact)\n",
        "                logger.info(f\"Loaded {len(contacts)} contacts.\")\n",
        "                return contacts\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error reconstructing contacts from JSON: {e}\", exc_info=True)\n",
        "                return None\n",
        "        else:\n",
        "            logger.warning(\"Failed to load or parse contacts JSON file.\")\n",
        "            return None\n",
        "\n",
        "    def _load_conversations_from_file(self) -> Optional[Dict[str, Conversation]]:\n",
        "        \"\"\"Load conversations from the JSON file.\"\"\"\n",
        "        if not os.path.exists(self.config.conversation_data_path):\n",
        "            logger.info(\"Conversations file not found, initializing empty dict.\")\n",
        "            return None\n",
        "        logger.info(f\"Loading existing conversations from: {self.config.conversation_data_path}\")\n",
        "        data = load_json_safe(self.config.conversation_data_path)\n",
        "        if data and isinstance(data, dict):\n",
        "            try:\n",
        "                # Reconstruct Conversation objects\n",
        "                conversations = {}\n",
        "                for conv_id, conv_dict in data.items():\n",
        "                     # Handle potential missing fields gracefully\n",
        "                    conv = Conversation(**{k: v for k, v in conv_dict.items() if k in Conversation.__annotations__})\n",
        "                    conversations[conv_id] = conv\n",
        "                logger.info(f\"Loaded {len(conversations)} conversations.\")\n",
        "                return conversations\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error reconstructing conversations from JSON: {e}\", exc_info=True)\n",
        "                return None\n",
        "        else:\n",
        "            logger.warning(\"Failed to load or parse conversations JSON file.\")\n",
        "            return None\n",
        "\n",
        "    def run_message_analysis(self):\n",
        "        \"\"\"Orchestrate the message analysis process.\"\"\"\n",
        "        self.supervisor.update_phase(\"message_analysis\")\n",
        "        logger.info(\"Starting message analysis phase...\")\n",
        "\n",
        "        # Get message files from database\n",
        "        self.supervisor.update_status(\"loading_messages\", \"Querying message files from DB\")\n",
        "        message_files_data = self._get_message_files_from_db()\n",
        "\n",
        "        if not message_files_data:\n",
        "            logger.warning(\"No message files found in database (category='Messages'). Skipping message analysis.\")\n",
        "            self.supervisor.update_status(\"skipped\", \"No message files found\")\n",
        "            return # Nothing to analyze\n",
        "\n",
        "        num_message_files = len(message_files_data)\n",
        "        self.supervisor.set_message_totals(num_message_files) # Total messages to process in this phase\n",
        "        logger.info(f\"Found {num_message_files} message files to analyze.\")\n",
        "        self.supervisor.update_status(\"analyzing_messages\", f\"Processing {num_message_files} messages\")\n",
        "\n",
        "        # --- Stage 1: Extract/Update Contacts and Conversations ---\n",
        "        # Process files, update contact map and conversation map incrementally\n",
        "        logger.info(\"Processing message files to update contacts and conversations...\")\n",
        "        contacts_map = {c.phone_number: c for c in self.contacts} # Use loaded contacts\n",
        "        conversations_map = self.conversations # Use loaded conversations\n",
        "\n",
        "        # Use ThreadPoolExecutor for I/O bound task of reading files + basic parsing\n",
        "        # Limit workers as parsing is also somewhat CPU intensive\n",
        "        num_workers = min(max(1, (self.config.parallel_jobs // 2)), 8) # Heuristic: fewer workers than file analysis\n",
        "        logger.info(f\"Using {num_workers} workers for message content processing.\")\n",
        "\n",
        "        processed_message_count = 0\n",
        "        all_message_metadata = {} # Store metadata to avoid reading files multiple times\n",
        "\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "                # Submit tasks to read and parse metadata\n",
        "                future_to_path = {executor.submit(self._read_message_content, msg_data['full_path']): msg_data['full_path'] for msg_data in message_files_data}\n",
        "\n",
        "                for future in as_completed(future_to_path):\n",
        "                    if stop_event.is_set(): break\n",
        "                    file_path = future_to_path[future]\n",
        "                    try:\n",
        "                        content, metadata = future.result()\n",
        "                        if metadata and not metadata.error:\n",
        "                            all_message_metadata[file_path] = metadata\n",
        "                        elif metadata and metadata.error:\n",
        "                            self.supervisor.log_warning(f\"Error reading message file {os.path.basename(file_path)}\", metadata.error)\n",
        "                            self.supervisor.update_stat(\"message_read_errors\", 1)\n",
        "                        else:\n",
        "                             self.supervisor.log_warning(f\"No metadata returned for {os.path.basename(file_path)}\")\n",
        "                             self.supervisor.update_stat(\"message_read_errors\", 1)\n",
        "\n",
        "                    except Exception as exc:\n",
        "                        logger.error(f'Message file {file_path} generated an exception: {exc}')\n",
        "                        self.supervisor.log_error(f\"Error processing message future for {os.path.basename(file_path)}\", str(exc))\n",
        "                        self.supervisor.update_stat(\"message_processing_errors\", 1)\n",
        "\n",
        "                    processed_message_count += 1\n",
        "                    if processed_message_count % 200 == 0:\n",
        "                         self.supervisor.update_message_progress(200)\n",
        "                         logger.info(f\"Read metadata for {processed_message_count}/{num_message_files} message files...\")\n",
        "\n",
        "            if stop_event.is_set():\n",
        "                 logger.warning(\"Message metadata reading interrupted.\")\n",
        "                 return # Abort phase if stopped\n",
        "\n",
        "            # Update supervisor progress for remaining files read\n",
        "            self.supervisor.update_message_progress(num_message_files - processed_message_count)\n",
        "            logger.info(f\"Finished reading metadata for {len(all_message_metadata)} files.\")\n",
        "            self.supervisor.update_status(\"analyzing_messages\", \"Updating contact and conversation data\")\n",
        "\n",
        "\n",
        "            # --- Stage 1b: Update data structures based on metadata ---\n",
        "            file_contents_by_contact = defaultdict(list) # For topic analysis later\n",
        "            file_contents_by_conv = defaultdict(list)\n",
        "\n",
        "            update_count = 0\n",
        "            for msg_data in message_files_data: # Iterate through original list to maintain order if needed\n",
        "                if stop_event.is_set(): break\n",
        "\n",
        "                file_path = msg_data['full_path']\n",
        "                metadata = all_message_metadata.get(file_path)\n",
        "\n",
        "                if not metadata or not metadata.phone_numbers:\n",
        "                    continue # Skip if error reading or no participants found\n",
        "\n",
        "                file_id = msg_data['id']\n",
        "                file_date = metadata.modification_time or msg_data['modification_date'] # Prefer metadata time\n",
        "\n",
        "                # Update Contacts\n",
        "                for number in metadata.phone_numbers:\n",
        "                    if number not in contacts_map:\n",
        "                        normalized_num = number # Placeholder\n",
        "                        if PHONENUMBERS_AVAILABLE:\n",
        "                            try:\n",
        "                                # Try parsing with default region (e.g., 'US'), then without region\n",
        "                                parsed_num = None\n",
        "                                possible_regions = ['US', None] # Add more default regions if relevant\n",
        "                                for region in possible_regions:\n",
        "                                     try:\n",
        "                                         parsed_num = parse_phone(number, region)\n",
        "                                         if parsed_num and is_valid_number(parsed_num):\n",
        "                                             normalized_num = format_number(parsed_num, PhoneNumberFormat.E164)\n",
        "                                             break # Found valid E.164 format\n",
        "                                         except NumberParseException:\n",
        "                                             continue # Try next region or fallback\n",
        "                                         except Exception as parse_exc: # Catch other potential phonenumbers errors\n",
        "                                             logger.warning(f\"Phonenumbers internal error for '{number}': {parse_exc}\")\n",
        "                                             continue # Try fallback\n",
        "\n",
        "                                if parsed_num and not is_valid_number(parsed_num):\n",
        "                                     # If parsed but not valid, maybe still add the cleaned version?\n",
        "                                     # For now, we prioritize valid numbers. Add cleaned if no valid found.\n",
        "                                     if cleaned_num not in numbers: # Avoid adding if E.164 was already added\n",
        "                                         pass # Don't add invalid numbers as contacts unless absolutely necessary\n",
        "                                elif not parsed_num: # If parsing failed completely\n",
        "                                     # If parsing failed, maybe add the cleaned number as a fallback?\n",
        "                                     # contacts_map[number] = Contact(phone_number=number, normalized_number=cleaned_num) # Consider adding if needed\n",
        "                                     pass # Skip if parsing failed and not a valid number\n",
        "\n",
        "                            except Exception as e:\n",
        "                                 logger.error(f\"Phonenumbers unexpected error for '{number}': {e}\")\n",
        "                                 # contacts_map[number] = Contact(phone_number=number, normalized_number=cleaned_num) # Fallback on error\n",
        "                                 pass # Skip on unexpected error\n",
        "                        # Add contact only if we have a potentially valid number\n",
        "                        if PHONENUMBERS_AVAILABLE and parsed_num and is_valid_number(parsed_num):\n",
        "                             contacts_map[number] = Contact(phone_number=number, normalized_number=normalized_num)\n",
        "                        elif not PHONENUMBERS_AVAILABLE and 6 < len(re.sub(r'\\D', '', number)) < 16:\n",
        "                             contacts_map[number] = Contact(phone_number=number, normalized_number=re.sub(r'\\D', '', number))\n",
        "                        else:\n",
        "                             continue # Skip if not a valid number and phonenumbers is available\n",
        "\n",
        "                    contact = contacts_map.get(number)\n",
        "                    if contact is None: continue # Should not happen if logic above is correct\n",
        "\n",
        "                    # Avoid double counting if file_id already processed for this contact\n",
        "                    if file_id not in contact.files:\n",
        "                        contact.message_count += 1\n",
        "                        contact.files.add(file_id)\n",
        "\n",
        "                        # Update sentiment (running average)\n",
        "                        if metadata.sentiment_score != 0:\n",
        "                             # More stable running average calculation\n",
        "                             contact.sentiment_score = ((contact.sentiment_score * (contact.message_count - 1)) + metadata.sentiment_score) / contact.message_count\n",
        "\n",
        "                        # Update first/last seen\n",
        "                        if contact.first_seen is None or (file_date and file_date < contact.first_seen):\n",
        "                            contact.first_seen = file_date\n",
        "                        if contact.last_seen is None or (file_date and file_date > contact.last_seen):\n",
        "                            contact.last_seen = file_date\n",
        "\n",
        "                        # Add location hints (unique)\n",
        "                        contact.location_hints.extend(l for l in metadata.entities.get('locations', []) if l not in contact.location_hints)\n",
        "\n",
        "                        # Store content for topic analysis (only if contact needs update)\n",
        "                        if metadata.content:\n",
        "                            file_contents_by_contact[number].append(metadata.content)\n",
        "\n",
        "                    # Track common contacts (always update this)\n",
        "                    for other_number in metadata.phone_numbers:\n",
        "                        if other_number != number:\n",
        "                            contact.common_contacts[other_number] = contact.common_contacts.get(other_number, 0) + 1\n",
        "\n",
        "\n",
        "                # Update Conversations\n",
        "                participants = sorted(metadata.phone_numbers)\n",
        "                if not participants: continue\n",
        "                conversation_key = \",\".join(participants)\n",
        "                is_group = len(participants) > 2 # Simple group chat definition\n",
        "\n",
        "                if conversation_key not in conversations_map:\n",
        "                    conversations_map[conversation_key] = Conversation(\n",
        "                        participants=participants,\n",
        "                        is_group_chat=is_group\n",
        "                    )\n",
        "\n",
        "                conversation = conversations_map[conversation_key]\n",
        "                # Check if file already exists in conversation to avoid duplicates\n",
        "                if not any(f['path'] == file_path for f in conversation.files):\n",
        "                    conversation.message_count += 1\n",
        "                    # Store file info with sentiment\n",
        "                    conversation.files.append({\n",
        "                        'path': file_path,\n",
        "                        'date': file_date,\n",
        "                        'sentiment': metadata.sentiment_score\n",
        "                    })\n",
        "\n",
        "                    # Update sentiment running average\n",
        "                    if metadata.sentiment_score != 0:\n",
        "                         conversation.sentiment_score = ((conversation.sentiment_score * (conversation.message_count - 1)) + metadata.sentiment_score) / conversation.message_count\n",
        "\n",
        "                    # Update first/last message dates\n",
        "                    if conversation.first_message is None or (file_date and file_date < conversation.first_message):\n",
        "                        conversation.first_message = file_date\n",
        "                    if conversation.last_message is None or (file_date and file_date > conversation.last_message):\n",
        "                        conversation.last_message = file_date\n",
        "\n",
        "                    # Store content for topic analysis\n",
        "                    if metadata.content:\n",
        "                         file_contents_by_conv[conversation_key].append(metadata.content)\n",
        "\n",
        "                update_count += 1\n",
        "                if update_count % 1000 == 0:\n",
        "                     logger.info(f\"Updated data structures for {update_count} messages...\")\n",
        "\n",
        "\n",
        "            if stop_event.is_set():\n",
        "                 logger.warning(\"Message data structure update interrupted.\")\n",
        "                 return\n",
        "\n",
        "            # --- Stage 1c: Topic Modeling ---\n",
        "            logger.info(\"Extracting topics for contacts and conversations...\")\n",
        "            self.supervisor.update_status(\"analyzing_messages\", \"Extracting topics\")\n",
        "\n",
        "            # Extract topics per contact (only for contacts with new content)\n",
        "            topic_processed_count = 0\n",
        "            for number, texts in file_contents_by_contact.items():\n",
        "                 if stop_event.is_set(): break\n",
        "                 if number in contacts_map and len(texts) >= 2: # Need at least 2 docs for TF-IDF min_df\n",
        "                     # Combine new texts with potentially existing ones if needed, or just analyze new ones\n",
        "                     # For simplicity, let's just analyze the content associated with this run's files\n",
        "                     topics = self._extract_topics(texts, num_topics=3)\n",
        "                     # Append topics, avoiding duplicates\n",
        "                     existing_topics = set(contacts_map[number].topics)\n",
        "                     contacts_map[number].topics = sorted(list(existing_topics.union(set(topics))))\n",
        "\n",
        "                 topic_processed_count += 1\n",
        "                 if topic_processed_count % 200 == 0:\n",
        "                      logger.info(f\"Processed topics for {topic_processed_count} contacts...\")\n",
        "\n",
        "            # Extract topics per conversation (only for conversations with new content)\n",
        "            topic_processed_count = 0\n",
        "            for conv_key, texts in file_contents_by_conv.items():\n",
        "                 if stop_event.is_set(): break\n",
        "                 if conv_key in conversations_map and len(texts) >= 2:\n",
        "                     topics = self._extract_topics(texts, num_topics=3)\n",
        "                     existing_topics = set(conversations_map[conv_key].topics)\n",
        "                     conversations_map[conv_key].topics = sorted(list(existing_topics.union(set(topics))))\n",
        "                     # Sort files within conversation by date after processing\n",
        "                     conversations_map[conv_key].files.sort(key=lambda x: (parse_iso_datetime(x.get('date')) or datetime.min))\n",
        "\n",
        "\n",
        "                 topic_processed_count += 1\n",
        "                 if topic_processed_count % 200 == 0:\n",
        "                      logger.info(f\"Processed topics for {topic_processed_count} conversations...\")\n",
        "\n",
        "\n",
        "            if stop_event.is_set():\n",
        "                 logger.warning(\"Topic modeling interrupted.\")\n",
        "                 return\n",
        "\n",
        "\n",
        "            # Update internal state\n",
        "            self.contacts = sorted(contacts_map.values(), key=lambda c: c.message_count, reverse=True)\n",
        "            self.conversations = conversations_map\n",
        "\n",
        "            logger.info(f\"Total contacts: {len(self.contacts)}, Total conversations: {len(self.conversations)}\")\n",
        "            self.supervisor.update_stat(\"contacts_total\", len(self.contacts))\n",
        "            self.supervisor.update_stat(\"conversations_total\", len(self.conversations))\n",
        "\n",
        "\n",
        "            # Filter conversations based on minimum file count\n",
        "            initial_conv_count = len(self.conversations)\n",
        "            self.conversations = {\n",
        "                conv_id: data\n",
        "                for conv_id, data in self.conversations.items()\n",
        "                if data.message_count >= self.config.min_conversation_files\n",
        "            }\n",
        "            filtered_conv_count = len(self.conversations)\n",
        "            if initial_conv_count != filtered_conv_count:\n",
        "                 logger.info(f\"Filtered conversations: {initial_conv_count} -> {filtered_conv_count} (min {self.config.min_conversation_files} files)\")\n",
        "                 self.supervisor.update_stat(\"conversations_filtered_out\", initial_conv_count - filtered_conv_count)\n",
        "\n",
        "\n",
        "            # --- Stage 2: Save Results ---\n",
        "            self.supervisor.update_status(\"saving_results\", \"Saving contacts and conversations\")\n",
        "            self._save_contacts()\n",
        "            self._save_conversations()\n",
        "            self._save_conversations_by_contact() # Save per-contact files\n",
        "\n",
        "            # --- Stage 3: Build and Visualize Contact Graph ---\n",
        "            if NETWORKX_AVAILABLE and MATPLOTLIB_AVAILABLE:\n",
        "                self.supervisor.update_status(\"building_graph\", \"Building contact network\")\n",
        "                # Pass contacts_map for efficient graph building\n",
        "                contact_graph = self._build_contact_graph(contacts_map)\n",
        "                if contact_graph and contact_graph.number_of_nodes() > 1:\n",
        "                    self.supervisor.update_status(\"visualizing_graph\", \"Generating contact graph image\")\n",
        "                    graph_path = os.path.join(self.config.visualization_dir, \"Graphs\", \"contact_network.png\")\n",
        "                    self._visualize_contact_graph(contact_graph, graph_path, top_n=self.config.graph_top_n)\n",
        "\n",
        "                    # Optional: Sentiment-based graph\n",
        "                    if self.sentiment_analyzer and any(abs(c.sentiment_score) > 0.05 for c in self.contacts):\n",
        "                         sentiment_graph_path = os.path.join(self.config.visualization_dir, \"Graphs\", \"contact_sentiment_network.png\")\n",
        "                         self._visualize_sentiment_graph(contact_graph, sentiment_graph_path, top_n=self.config.graph_top_n)\n",
        "                else:\n",
        "                     logger.warning(\"Contact graph has too few nodes (<2) or failed to build, skipping visualization.\")\n",
        "            else:\n",
        "                logger.warning(\"NetworkX or Matplotlib not available, skipping graph analysis and visualization.\")\n",
        "\n",
        "            # --- Stage 4: Generate Statistics ---\n",
        "            # This might be better integrated into reporting or visualization stages\n",
        "            # self.supervisor.update_status(\"generating_stats\", \"Generating message statistics\")\n",
        "            # self._generate_contact_statistics() # Keep simple stats for now\n",
        "\n",
        "            logger.info(\"Message analysis phase completed.\")\n",
        "            self.supervisor.update_status(\"completed\", \"Message analysis finished\")\n",
        "\n",
        "        except InterruptedError: # Catch interruption from signal handler\n",
        "             logger.warning(\"Message analysis phase interrupted by stop signal.\")\n",
        "             self.supervisor.update_status(\"interrupted\", \"Message analysis stopped\")\n",
        "        except Exception as e:\n",
        "            logger.critical(f\"Critical error during message analysis: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Critical error in message analysis\", str(e))\n",
        "            self.supervisor.update_status(\"failed\", \"Critical message analysis error\")\n",
        "\n",
        "\n",
        "    def _get_message_files_from_db(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Get message file details from the database.\"\"\"\n",
        "        # Select files categorized as 'Messages' or 'Contacts' (like VCF) if needed\n",
        "        # For now, focus on 'Messages' category as defined by FileAnalyzer\n",
        "        query = \"\"\"\n",
        "            SELECT id, path, filename, creation_date, modification_date\n",
        "            FROM files\n",
        "            WHERE category = 'Messages' AND error IS NULL\n",
        "            ORDER BY modification_date ASC -- Process in rough chronological order\n",
        "        \"\"\"\n",
        "        try:\n",
        "            message_files_rows = self.db_manager.execute_query(query)\n",
        "            message_files = []\n",
        "            checked_paths = set() # Avoid checking existence repeatedly for same path\n",
        "            missing_files_count = 0\n",
        "\n",
        "            for row in message_files_rows:\n",
        "                full_path = os.path.join(row['path'], row['filename'])\n",
        "\n",
        "                # Check existence efficiently\n",
        "                dir_path = row['path']\n",
        "                if dir_path not in checked_paths:\n",
        "                     if not os.path.isdir(dir_path):\n",
        "                         logger.warning(f\"Directory not found for message files: {dir_path}. Skipping files within.\")\n",
        "                         # Add to checked paths to avoid re-checking\n",
        "                         checked_paths.add(dir_path)\n",
        "                         # We could potentially skip all files from this dir here\n",
        "                         continue # Skip this file if dir doesn't exist\n",
        "                     checked_paths.add(dir_path) # Mark dir as checked\n",
        "\n",
        "                # Check file existence\n",
        "                if os.path.isfile(full_path):\n",
        "                    message_files.append({\n",
        "                        'id': row['id'],\n",
        "                        'path': row['path'],\n",
        "                        'filename': row['filename'],\n",
        "                        'full_path': full_path,\n",
        "                        'creation_date': row['creation_date'],\n",
        "                        'modification_date': row['modification_date'],\n",
        "                        # Phone numbers will be extracted from content/metadata later\n",
        "                    })\n",
        "                else:\n",
        "                    logger.warning(f\"Message file listed in DB not found on disk: {full_path}\")\n",
        "                    missing_files_count += 1\n",
        "                    # Optionally mark the file as having an error in the DB?\n",
        "                    # self.db_manager.execute_write(\"UPDATE files SET error = ? WHERE id = ?\", (\"File not found on disk\", row['id']))\n",
        "\n",
        "\n",
        "            if missing_files_count > 0:\n",
        "                 self.supervisor.log_warning(f\"{missing_files_count} message files from DB were not found on disk.\")\n",
        "                 self.supervisor.update_stat(\"message_files_missing_on_disk\", missing_files_count)\n",
        "\n",
        "            return message_files\n",
        "        except InterruptedError:\n",
        "             raise # Propagate interruption\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error retrieving message files from database: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Failed to get message files from DB\", str(e))\n",
        "            return []\n",
        "\n",
        "    def _read_message_content(self, file_path: str) -> Tuple[str, Optional[MessageMetadata]]:\n",
        "        \"\"\"Read message content and extract metadata (run in thread/process).\"\"\"\n",
        "        metadata = MessageMetadata(filename=os.path.basename(file_path), file_path=file_path)\n",
        "        content = \"\"\n",
        "        try:\n",
        "            # Determine encoding - UTF-8 is common, but fallbacks might be needed\n",
        "            encodings_to_try = ['utf-8', 'latin-1', 'cp1252']\n",
        "            for enc in encodings_to_try:\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding=enc) as f:\n",
        "                        content = f.read()\n",
        "                    logger.debug(f\"Read {file_path} with encoding {enc}\")\n",
        "                    break # Stop if successful\n",
        "                except UnicodeDecodeError:\n",
        "                    logger.debug(f\"Failed to decode {file_path} with {enc}\")\n",
        "                    continue # Try next encoding\n",
        "                except Exception as read_err: # Catch other file reading errors\n",
        "                     raise read_err # Re-raise other errors like permission denied\n",
        "            else: # If loop completes without break\n",
        "                 logger.warning(f\"Could not decode file {file_path} with tried encodings. Reading as binary.\")\n",
        "                 # Fallback: read as binary and decode with error handling\n",
        "                 try:\n",
        "                     with open(file_path, 'rb') as f:\n",
        "                          binary_content = f.read()\n",
        "                     content = binary_content.decode('utf-8', errors='ignore') # Decode ignoring errors\n",
        "                 except Exception as bin_read_err:\n",
        "                     raise bin_read_err # Re-raise if binary read fails\n",
        "\n",
        "\n",
        "            stats = os.stat(file_path)\n",
        "            try: # Platform specific creation time\n",
        "                metadata.creation_time = datetime.fromtimestamp(stats.st_birthtime).isoformat()\n",
        "            except AttributeError:\n",
        "                metadata.creation_time = datetime.fromtimestamp(stats.st_ctime).isoformat()\n",
        "            metadata.modification_time = datetime.fromtimestamp(stats.st_mtime).isoformat()\n",
        "            metadata.content = content # Store content in metadata\n",
        "\n",
        "            # Extract entities from filename and content\n",
        "            # Use the global extract_phone_numbers utility\n",
        "            filename_phones = extract_phone_numbers(metadata.filename, None) # Use None logger in worker\n",
        "            content_phones = extract_phone_numbers(content, None)\n",
        "            all_phones = sorted(list(set(filename_phones + content_phones)))\n",
        "            metadata.phone_numbers = all_phones\n",
        "\n",
        "            # Extract other entities (simplified for now)\n",
        "            metadata.timestamps = re.findall(r'\\d{4}-\\d{2}-\\d{2}[ T]\\d{2}:\\d{2}:\\d{2}', content) # Basic ISO timestamp\n",
        "\n",
        "            # Analyze sentiment\n",
        "            if self.sentiment_analyzer and len(content) > 10: # Avoid analyzing very short strings\n",
        "                 metadata.sentiment_score = self._analyze_sentiment(content)\n",
        "\n",
        "            return content, metadata\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            metadata.error = \"File not found\"\n",
        "            # logger already handled in the calling function\n",
        "        except OSError as e:\n",
        "            metadata.error = f\"OS Error: {e}\"\n",
        "        except Exception as e:\n",
        "            metadata.error = f\"Unexpected Error: {e}\"\n",
        "            logger.error(f\"Unexpected error reading message file {file_path}: {e}\", exc_info=True) # Log details here\n",
        "\n",
        "        return content, metadata # Return metadata with error flag set\n",
        "\n",
        "    def _analyze_sentiment(self, text: str) -> float:\n",
        "        \"\"\"Analyze sentiment using VADER.\"\"\"\n",
        "        # Assumes self.sentiment_analyzer is initialized if SENTIMENT_AVAILABLE\n",
        "        if not self.sentiment_analyzer: return 0.0\n",
        "        try:\n",
        "            vs = self.sentiment_analyzer.polarity_scores(text)\n",
        "            return vs['compound'] # Compound score: -1 (neg) to +1 (pos)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Sentiment analysis failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _extract_topics(self, texts: List[str], num_topics: int = 5) -> List[str]:\n",
        "        \"\"\"Extract topics using TF-IDF (simplified).\"\"\"\n",
        "        if not SKLEARN_AVAILABLE or not texts or len(texts) < 2: # Need at least 2 documents for min_df\n",
        "            return []\n",
        "        # Filter out very short texts\n",
        "        texts = [t for t in texts if len(t.split()) > 5]\n",
        "        if len(texts) < 2: return []\n",
        "\n",
        "        try:\n",
        "            # Use TfidfVectorizer with common parameters\n",
        "            vectorizer = TfidfVectorizer(\n",
        "                max_df=0.85,         # Ignore terms appearing in > 85% of docs\n",
        "                min_df=2,            # Ignore terms appearing in < 2 docs\n",
        "                stop_words='english',\n",
        "                max_features=1000,   # Limit vocabulary size\n",
        "                ngram_range=(1, 2)   # Consider unigrams and bigrams\n",
        "            )\n",
        "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "            feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "            # Sum TF-IDF scores for each term across all documents in the batch\n",
        "            total_scores = tfidf_matrix.sum(axis=0).A1 # .A1 converts sparse matrix row to 1D numpy array\n",
        "\n",
        "            # Get indices of top N scores, ensuring we don't request more topics than features\n",
        "            actual_num_topics = min(num_topics, len(feature_names))\n",
        "            if actual_num_topics == 0: return []\n",
        "\n",
        "            top_indices = total_scores.argsort()[-actual_num_topics:][::-1] # Get indices of top N scores\n",
        "\n",
        "            return feature_names[top_indices].tolist()\n",
        "        except ValueError as ve:\n",
        "             # Handle specific sklearn errors, e.g., empty vocabulary\n",
        "             if \"empty vocabulary\" in str(ve):\n",
        "                  logger.warning(f\"Topic extraction skipped: Empty vocabulary after filtering.\")\n",
        "             else:\n",
        "                  logger.error(f\"Topic extraction ValueError: {ve}\", exc_info=True)\n",
        "             return []\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Topic extraction failed: {e}\", exc_info=True)\n",
        "            return []\n",
        "\n",
        "    def _save_contacts(self):\n",
        "        \"\"\"Save extracted contact data to JSON.\"\"\"\n",
        "        output_path = self.config.contact_data_path\n",
        "        logger.info(f\"Saving {len(self.contacts)} contacts to: {output_path}\")\n",
        "        try:\n",
        "            contacts_dict_list = [c.to_dict() for c in self.contacts]\n",
        "            atomic_write_json(contacts_dict_list, output_path)\n",
        "            # Optionally save checksum\n",
        "            # checksum = calculate_checksum(output_path)\n",
        "            # if checksum: atomic_write_json({\"checksum\": checksum}, f\"{output_path}.checksum\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save contacts JSON: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Failed to save contacts JSON\", str(e))\n",
        "\n",
        "    def _save_conversations(self):\n",
        "        \"\"\"Save filtered conversation data to JSON.\"\"\"\n",
        "        output_path = self.config.conversation_data_path\n",
        "        logger.info(f\"Saving {len(self.conversations)} filtered conversations to: {output_path}\")\n",
        "        try:\n",
        "            # Convert Conversation objects to dictionaries for saving\n",
        "            conversations_dict = {k: v.to_dict() for k, v in self.conversations.items()}\n",
        "            atomic_write_json(conversations_dict, output_path)\n",
        "            # Optionally save checksum\n",
        "            # checksum = calculate_checksum(output_path)\n",
        "            # if checksum: atomic_write_json({\"checksum\": checksum}, f\"{output_path}.checksum\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save conversations JSON: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Failed to save conversations JSON\", str(e))\n",
        "\n",
        "    def _save_conversations_by_contact(self):\n",
        "        \"\"\"Save conversations associated with each contact to individual files.\"\"\"\n",
        "        output_dir = self.config.contact_conv_dir\n",
        "        logger.info(f\"Saving individual conversation files to: {output_dir}\")\n",
        "        saved_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        # Create a map of contact number -> list of conversation keys they participate in\n",
        "        contact_to_conv_keys = defaultdict(list)\n",
        "        for conv_key, conv_data in self.conversations.items():\n",
        "            for participant in conv_data.participants:\n",
        "                 contact_to_conv_keys[participant].append(conv_key)\n",
        "\n",
        "        # Iterate through contacts and save their conversations\n",
        "        for contact in self.contacts:\n",
        "            if stop_event.is_set(): break\n",
        "\n",
        "            contact_number = contact.phone_number\n",
        "            conv_keys = contact_to_conv_keys.get(contact_number)\n",
        "\n",
        "            if not conv_keys:\n",
        "                skipped_count += 1\n",
        "                continue # Skip contacts with no associated conversations (after filtering)\n",
        "\n",
        "            # Prepare data for the contact's file\n",
        "            contact_conversations = {}\n",
        "            for key in conv_keys:\n",
        "                if key in self.conversations:\n",
        "                    # Include conversation data, maybe simplify file list?\n",
        "                    conv_data = self.conversations[key].to_dict()\n",
        "                    # Optional: Limit file list length in output JSON?\n",
        "                    # conv_data['files'] = conv_data['files'][:50] # Example limit\n",
        "                    contact_conversations[key] = conv_data\n",
        "\n",
        "            if not contact_conversations:\n",
        "                 skipped_count += 1\n",
        "                 continue\n",
        "\n",
        "            # Sanitize phone number for filename\n",
        "            safe_filename = sanitize_filename(f\"contact_{contact_number}.json\")\n",
        "            output_path = os.path.join(output_dir, safe_filename)\n",
        "\n",
        "            try:\n",
        "                atomic_write_json(contact_conversations, output_path)\n",
        "                saved_count += 1\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to save conversation file for contact {contact_number}: {e}\")\n",
        "                self.supervisor.log_error(f\"Failed to save conv file for {contact_number}\", str(e))\n",
        "                skipped_count += 1\n",
        "\n",
        "            if saved_count % 100 == 0 and saved_count > 0:\n",
        "                 logger.info(f\"Saved {saved_count} contact conversation files...\")\n",
        "\n",
        "        logger.info(f\"Finished saving individual conversation files. Saved: {saved_count}, Skipped/Empty: {skipped_count}\")\n",
        "        self.supervisor.update_stat(\"contact_conversation_files_saved\", saved_count)\n",
        "\n",
        "\n",
        "    def _build_contact_graph(self, contacts_map: Dict[str, Contact]) -> Optional[nx.Graph]:\n",
        "        \"\"\"Build a NetworkX graph from contact interactions.\"\"\"\n",
        "        if not NETWORKX_AVAILABLE: return None\n",
        "        logger.info(\"Building contact interaction graph...\")\n",
        "        G = nx.Graph()\n",
        "\n",
        "        # Add nodes (contacts) with attributes\n",
        "        for number, contact in contacts_map.items():\n",
        "             if contact.message_count > 0: # Only add nodes with messages\n",
        "                 G.add_node(\n",
        "                     number, # Use phone number as node ID\n",
        "                     message_count=contact.message_count,\n",
        "                     sentiment=contact.sentiment_score,\n",
        "                     first_seen=contact.first_seen,\n",
        "                     last_seen=contact.last_seen,\n",
        "                     normalized_number=contact.normalized_number\n",
        "                 )\n",
        "\n",
        "        # Add edges based on common contacts or conversations\n",
        "        # Method 2: Edges based on participation in the same conversation (more direct link)\n",
        "        for conv_key, conversation in self.conversations.items():\n",
        "             participants = conversation.participants\n",
        "             # Only add edges between participants present in the graph nodes\n",
        "             valid_participants = [p for p in participants if p in G]\n",
        "             if len(valid_participants) > 1:\n",
        "                 # Add edges between all pairs in this conversation\n",
        "                 for i in range(len(valid_participants)):\n",
        "                     for j in range(i + 1, len(valid_participants)):\n",
        "                         u, v = valid_participants[i], valid_participants[j]\n",
        "                         # Weight edge by number of messages in this conversation? Or just +1 interaction?\n",
        "                         weight = conversation.message_count # Weight by conversation size\n",
        "                         if G.has_edge(u, v):\n",
        "                             G[u][v]['weight'] += weight\n",
        "                             G[u][v]['conversations'] = G[u][v].get('conversations', 0) + 1\n",
        "                         else:\n",
        "                             G.add_edge(u, v, weight=weight, conversations=1)\n",
        "\n",
        "\n",
        "        logger.info(f\"Built contact graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
        "        self.supervisor.update_stat(\"graph_nodes\", G.number_of_nodes())\n",
        "        self.supervisor.update_stat(\"graph_edges\", G.number_of_edges())\n",
        "        return G\n",
        "\n",
        "    def _visualize_contact_graph(self, G: nx.Graph, output_path: str, top_n: int = 30):\n",
        "        \"\"\"Visualize the contact graph using Matplotlib.\"\"\"\n",
        "        if not NETWORKX_AVAILABLE or not MATPLOTLIB_AVAILABLE or G.number_of_nodes() == 0:\n",
        "            logger.warning(\"Graph visualization skipped (libs unavailable or empty graph).\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Generating contact graph visualization (top {top_n} nodes by degree) to: {output_path}\")\n",
        "\n",
        "        try:\n",
        "            # --- Subgraph for Visualization ---\n",
        "            # Select top N nodes based on degree (number of connections) or message_count? Let's use degree.\n",
        "            if G.number_of_nodes() > top_n:\n",
        "                top_nodes = sorted(G.degree(weight='weight'), key=lambda item: item[1], reverse=True)[:top_n]\n",
        "                top_node_ids = [node_id for node_id, degree in top_nodes]\n",
        "                subgraph = G.subgraph(top_node_ids)\n",
        "                logger.info(f\"Visualizing subgraph of {subgraph.number_of_nodes()} nodes.\")\n",
        "            else:\n",
        "                subgraph = G\n",
        "                logger.info(f\"Visualizing full graph with {subgraph.number_of_nodes()} nodes.\")\n",
        "\n",
        "            if subgraph.number_of_nodes() == 0:\n",
        "                 logger.warning(\"Subgraph for visualization is empty, skipping plot.\")\n",
        "                 return\n",
        "\n",
        "            # --- Layout ---\n",
        "            plt.figure(figsize=(18, 18)) # Increase figure size\n",
        "            # Use a layout algorithm suitable for social networks\n",
        "            # pos = nx.spring_layout(subgraph, k=0.5, iterations=50, seed=42) # Increase k for more spread\n",
        "            pos = nx.kamada_kawai_layout(subgraph) # Often good for smaller graphs\n",
        "\n",
        "            # --- Node Styling ---\n",
        "            node_sizes = [subgraph.nodes[n].get('message_count', 10) * 5 + 50 for n in subgraph.nodes()] # Size by message count\n",
        "            # Normalize node sizes to avoid extremes\n",
        "            max_size = 5000\n",
        "            min_size = 100\n",
        "            node_sizes = [min(max(s, min_size), max_size) for s in node_sizes]\n",
        "\n",
        "            # --- Edge Styling ---\n",
        "            # Use edge weight for width or transparency? Let's use transparency (alpha).\n",
        "            weights = [subgraph[u][v].get('weight', 1) for u, v in subgraph.edges()]\n",
        "            max_weight = max(weights) if weights else 1\n",
        "            edge_alphas = [min(0.8, max(0.1, w / max_weight * 0.8)) for w in weights] # Scale alpha\n",
        "            edge_colors = 'grey'\n",
        "\n",
        "            # --- Drawing ---\n",
        "            nx.draw_networkx_nodes(subgraph, pos, node_size=node_sizes, node_color='skyblue', alpha=0.8)\n",
        "            nx.draw_networkx_edges(subgraph, pos, width=1.0, alpha=edge_alphas, edge_color=edge_colors)\n",
        "\n",
        "            # --- Labels ---\n",
        "            # Label only the most central nodes or nodes with high message count?\n",
        "            # Label top N/2 nodes by degree within the subgraph\n",
        "            labels = {n: n for idx, (n, deg) in enumerate(sorted(subgraph.degree(weight='weight'), key=lambda item: item[1], reverse=True)) if idx < top_n // 2}\n",
        "            nx.draw_networkx_labels(subgraph, pos, labels=labels, font_size=10)\n",
        "\n",
        "            # --- Final Touches ---\n",
        "            plt.title(f\"Contact Network (Top {subgraph.number_of_nodes()} Nodes by Weighted Degree)\", fontsize=16)\n",
        "            plt.axis('off') # Hide axes\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close() # Close the figure to free memory\n",
        "            logger.info(f\"Contact graph visualization saved to {output_path}\")\n",
        "            self.supervisor.update_stat(\"visualizations_created\", 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to visualize contact graph: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Contact graph visualization failed\", str(e))\n",
        "            self.supervisor.update_stat(\"visualization_errors\", 1)\n",
        "\n",
        "\n",
        "    def _visualize_sentiment_graph(self, G: nx.Graph, output_path: str, top_n: int = 30):\n",
        "        \"\"\"Visualize the contact graph with nodes colored by sentiment.\"\"\"\n",
        "        if not NETWORKX_AVAILABLE or not MATPLOTLIB_AVAILABLE or G.number_of_nodes() == 0 or not self.sentiment_analyzer:\n",
        "            logger.warning(\"Sentiment graph visualization skipped (libs unavailable, empty graph, or sentiment disabled).\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Generating contact sentiment graph visualization (top {top_n} nodes) to: {output_path}\")\n",
        "\n",
        "        try:\n",
        "            # --- Subgraph for Visualization ---\n",
        "            # Select top N nodes based on degree (same as main graph)\n",
        "            if G.number_of_nodes() > top_n:\n",
        "                top_nodes = sorted(G.degree(weight='weight'), key=lambda item: item[1], reverse=True)[:top_n]\n",
        "                top_node_ids = [node_id for node_id, degree in top_nodes]\n",
        "                subgraph = G.subgraph(top_node_ids)\n",
        "                logger.info(f\"Visualizing sentiment subgraph of {subgraph.number_of_nodes()} nodes.\")\n",
        "            else:\n",
        "                subgraph = G\n",
        "                logger.info(f\"Visualizing full sentiment graph with {subgraph.number_of_nodes()} nodes.\")\n",
        "\n",
        "            if subgraph.number_of_nodes() == 0:\n",
        "                 logger.warning(\"Subgraph for sentiment visualization is empty, skipping plot.\")\n",
        "                 return\n",
        "\n",
        "            # --- Layout ---\n",
        "            plt.figure(figsize=(18, 18)) # Increase figure size\n",
        "            pos = nx.kamada_kawai_layout(subgraph)\n",
        "\n",
        "            # --- Node Styling (Color by Sentiment) ---\n",
        "            # Map sentiment scores to colors (e.g., using a colormap)\n",
        "            sentiment_scores = np.array([subgraph.nodes[n].get('sentiment', 0.0) for n in subgraph.nodes()])\n",
        "\n",
        "            # Normalize sentiment scores to range [0, 1] for colormap\n",
        "            # VADER compound score is [-1, 1]. Map to [0, 1]\n",
        "            normalized_sentiment = (sentiment_scores + 1) / 2\n",
        "\n",
        "            # Use a diverging colormap (e.g., 'coolwarm', 'seismic')\n",
        "            cmap = plt.cm.coolwarm # Blue for negative, Red for positive\n",
        "            node_colors = cmap(normalized_sentiment)\n",
        "\n",
        "            # Node sizes (same as main graph)\n",
        "            node_sizes = [subgraph.nodes[n].get('message_count', 10) * 5 + 50 for n in subgraph.nodes()]\n",
        "            max_size = 5000\n",
        "            min_size = 100\n",
        "            node_sizes = [min(max(s, min_size), max_size) for s in node_sizes]\n",
        "\n",
        "            # --- Edge Styling (same as main graph) ---\n",
        "            weights = [subgraph[u][v].get('weight', 1) for u, v in subgraph.edges()]\n",
        "            max_weight = max(weights) if weights else 1\n",
        "            edge_alphas = [min(0.8, max(0.1, w / max_weight * 0.8)) for w in weights]\n",
        "            edge_colors = 'grey'\n",
        "\n",
        "\n",
        "            # --- Drawing ---\n",
        "            nodes = nx.draw_networkx_nodes(subgraph, pos, node_size=node_sizes, node_color=node_colors, alpha=0.8)\n",
        "            nx.draw_networkx_edges(subgraph, pos, width=1.0, alpha=edge_alphas, edge_color=edge_colors)\n",
        "\n",
        "            # --- Labels ---\n",
        "            labels = {n: n for idx, (n, deg) in enumerate(sorted(subgraph.degree(weight='weight'), key=lambda item: item[1], reverse=True)) if idx < top_n // 2}\n",
        "            nx.draw_networkx_labels(subgraph, pos, labels=labels, font_size=10)\n",
        "\n",
        "            # --- Add Colorbar for Sentiment ---\n",
        "            if len(np.unique(normalized_sentiment)) > 1: # Only add colorbar if there's variation\n",
        "                sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=-1, vmax=1))\n",
        "                sm.set_array([]) # Dummy array for colorbar\n",
        "                cbar = plt.colorbar(sm, label=\"Average Sentiment Score (Compound)\", orientation=\"horizontal\", shrink=0.5, pad=0.05)\n",
        "                cbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "\n",
        "            # --- Final Touches ---\n",
        "            plt.title(f\"Contact Sentiment Network (Top {subgraph.number_of_nodes()} Nodes)\", fontsize=16)\n",
        "            plt.axis('off') # Hide axes\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close() # Close the figure to free memory\n",
        "            logger.info(f\"Contact sentiment graph visualization saved to {output_path}\")\n",
        "            self.supervisor.update_stat(\"visualizations_created\", 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to visualize contact sentiment graph: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Contact sentiment graph visualization failed\", str(e))\n",
        "            self.supervisor.update_stat(\"visualization_errors\", 1)\n",
        "\n",
        "\n",
        "    # Placeholder for generating statistics (can be expanded or moved to ReportGenerator)\n",
        "    def _generate_contact_statistics(self):\n",
        "        \"\"\"Generate basic contact statistics.\"\"\"\n",
        "        logger.info(\"Generating contact statistics...\")\n",
        "        # Example: Count contacts by message count ranges\n",
        "        message_counts = [c.message_count for c in self.contacts]\n",
        "        if message_counts:\n",
        "            avg_messages = np.mean(message_counts)\n",
        "            total_messages = sum(message_counts)\n",
        "            logger.info(f\"Average messages per contact: {avg_messages:.2f}\")\n",
        "            logger.info(f\"Total messages processed: {total_messages}\")\n",
        "            self.supervisor.update_stat(\"avg_messages_per_contact\", avg_messages)\n",
        "            self.supervisor.update_stat(\"total_messages_processed\", total_messages)\n",
        "        else:\n",
        "             logger.info(\"No contacts with messages found for statistics.\")\n",
        "\n",
        "        # Example: Count contacts by sentiment ranges\n",
        "        if SENTIMENT_AVAILABLE:\n",
        "            positive_count = sum(1 for c in self.contacts if c.sentiment_score > 0.1)\n",
        "            negative_count = sum(1 for c in self.contacts if c.sentiment_score < -0.1)\n",
        "            neutral_count = len(self.contacts) - positive_count - negative_count\n",
        "            logger.info(f\"Contacts by sentiment: Positive={positive_count}, Negative={negative_count}, Neutral={neutral_count}\")\n",
        "            self.supervisor.update_stat(\"contacts_positive_sentiment\", positive_count)\n",
        "            self.supervisor.update_stat(\"contacts_negative_sentiment\", negative_count)\n",
        "            self.supervisor.update_stat(\"contacts_neutral_sentiment\", neutral_count)\n",
        "\n",
        "\n",
        "    # Placeholder for generating statistics (can be expanded or moved to ReportGenerator)\n",
        "    def _generate_conversation_statistics(self):\n",
        "        \"\"\"Generate basic conversation statistics.\"\"\"\n",
        "        logger.info(\"Generating conversation statistics...\")\n",
        "        # Example: Count conversations by number of participants, message count\n",
        "        conv_counts = [c.message_count for c in self.conversations.values()]\n",
        "        if conv_counts:\n",
        "            avg_conv_messages = np.mean(conv_counts)\n",
        "            total_conv_messages = sum(conv_counts)\n",
        "            logger.info(f\"Average messages per conversation: {avg_conv_messages:.2f}\")\n",
        "            logger.info(f\"Total messages in conversations: {total_conv_messages}\")\n",
        "            self.supervisor.update_stat(\"avg_messages_per_conversation\", avg_conv_messages)\n",
        "            self.supervisor.update_stat(\"total_messages_in_conversations\", total_conv_messages)\n",
        "\n",
        "        group_chats = sum(1 for c in self.conversations.values() if c.is_group_chat)\n",
        "        private_chats = len(self.conversations) - group_chats\n",
        "        logger.info(f\"Conversations: Group={group_chats}, Private={private_chats}\")\n",
        "        self.supervisor.update_stat(\"group_chats\", group_chats)\n",
        "        self.supervisor.update_stat(\"private_chats\", private_chats)\n",
        "\n",
        "    # Placeholder for generating statistics (can be expanded or moved to ReportGenerator)\n",
        "    def _generate_category_statistics(self):\n",
        "        \"\"\"Generate statistics on file categories.\"\"\"\n",
        "        logger.info(\"Generating file category statistics...\")\n",
        "        # Query category counts from the database\n",
        "        query = \"SELECT category, COUNT(*) as count FROM files GROUP BY category ORDER BY count DESC\"\n",
        "        try:\n",
        "            category_counts = self.db_manager.execute_query(query)\n",
        "            logger.info(\"File counts by category:\")\n",
        "            for row in category_counts:\n",
        "                logger.info(f\"  {row['category']}: {row['count']}\")\n",
        "                self.supervisor.update_stat(f\"files_category_{row['category']}\", row['count'])\n",
        "\n",
        "            # Also get total file count from DB\n",
        "            total_files_in_db = self.db_manager.execute_query(\"SELECT COUNT(*) as count FROM files\")\n",
        "            if total_files_in_db:\n",
        "                 total_files = total_files_in_db[0]['count']\n",
        "                 logger.info(f\"Total files in database: {total_files}\")\n",
        "                 self.supervisor.update_stat(\"files_total_in_db\", total_files)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate category statistics: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Category statistics failed\", str(e))\n",
        "\n",
        "    # Placeholder for generating statistics (can be expanded or moved to ReportGenerator)\n",
        "    def _generate_pattern_statistics(self):\n",
        "        \"\"\"Generate statistics on detected patterns.\"\"\"\n",
        "        logger.info(\"Generating pattern statistics...\")\n",
        "        # Query pattern counts from the database\n",
        "        query = \"SELECT pattern_type, pattern_value, COUNT(*) as count FROM patterns GROUP BY pattern_type, pattern_value ORDER BY count DESC\"\n",
        "        try:\n",
        "            pattern_counts = self.db_manager.execute_query(query)\n",
        "            logger.info(\"Pattern counts:\")\n",
        "            # Group by pattern type for better readability\n",
        "            patterns_by_type = defaultdict(list)\n",
        "            for row in pattern_counts:\n",
        "                patterns_by_type[row['pattern_type']].append((row['pattern_value'], row['count']))\n",
        "\n",
        "            for p_type, patterns in patterns_by_type.items():\n",
        "                logger.info(f\"  {p_type}:\")\n",
        "                for value, count in patterns[:self.config.report_top_n]: # Show top N patterns per type\n",
        "                     logger.info(f\"    {value}: {count}\")\n",
        "                     # Optionally store top patterns in supervisor stats\n",
        "                     # self.supervisor.update_stat(f\"pattern_{p_type}_{value}\", count) # Could lead to too many stats\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate pattern statistics: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Pattern statistics failed\", str(e))\n",
        "\n",
        "\n",
        "# --- Visualization Generator ---\n",
        "\n",
        "class VisualizationGenerator:\n",
        "    \"\"\"Generates various data visualizations.\"\"\"\n",
        "    def __init__(self, config: Config, db_manager: DatabaseManager, supervisor: AnalysisSupervisor):\n",
        "        self.config = config\n",
        "        self.db_manager = db_manager\n",
        "        self.supervisor = supervisor\n",
        "        # Ensure matplotlib is using the Agg backend\n",
        "        matplotlib.use('Agg')\n",
        "        # Initialize plot styling if seaborn is available\n",
        "        if SEABORN_AVAILABLE:\n",
        "            sns.set_theme(style=\"whitegrid\")\n",
        "            sns.set_palette(\"viridis\") # Example palette\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        \"\"\"Orchestrate the visualization generation process.\"\"\"\n",
        "        self.supervisor.update_phase(\"visualization\")\n",
        "        logger.info(\"Starting visualization generation phase...\")\n",
        "\n",
        "        if not MATPLOTLIB_AVAILABLE:\n",
        "            logger.warning(\"Matplotlib not available, skipping all visualizations.\")\n",
        "            self.supervisor.update_status(\"skipped\", \"Matplotlib not available\")\n",
        "            return\n",
        "\n",
        "        # Ensure visualization directories exist\n",
        "        os.makedirs(os.path.join(self.config.visualization_dir, \"Charts\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.config.visualization_dir, \"WordClouds\"), exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # Generate file category distribution chart\n",
        "            self.supervisor.update_status(\"generating_charts\", \"Generating file category chart\")\n",
        "            self._generate_category_chart()\n",
        "\n",
        "            # Generate file size distribution chart\n",
        "            self.supervisor.update_status(\"generating_charts\", \"Generating file size chart\")\n",
        "            self._generate_filesize_chart()\n",
        "\n",
        "            # Generate file modification date distribution chart\n",
        "            self.supervisor.update_status(\"generating_charts\", \"Generating modification date chart\")\n",
        "            self._generate_modification_date_chart()\n",
        "\n",
        "            # Generate sentiment distribution chart (if sentiment available)\n",
        "            if SENTIMENT_AVAILABLE and PANDAS_AVAILABLE:\n",
        "                self.supervisor.update_status(\"generating_charts\", \"Generating sentiment distribution chart\")\n",
        "                self._generate_sentiment_distribution_chart()\n",
        "\n",
        "            # Generate word cloud from message content (if libraries available)\n",
        "            if WORDCLOUD_AVAILABLE and PANDAS_AVAILABLE and NUMPY_AVAILABLE and SKLEARN_AVAILABLE:\n",
        "                 self.supervisor.update_status(\"generating_wordclouds\", \"Generating word cloud\")\n",
        "                 self._generate_message_wordcloud()\n",
        "\n",
        "            # Save visualization data (e.g., counts, distributions) to a JSON file\n",
        "            self._save_visualization_data()\n",
        "\n",
        "\n",
        "            logger.info(\"Visualization generation phase completed.\")\n",
        "            self.supervisor.update_status(\"completed\", \"Visualizations finished\")\n",
        "\n",
        "        except InterruptedError:\n",
        "             logger.warning(\"Visualization phase interrupted by stop signal.\")\n",
        "             self.supervisor.update_status(\"interrupted\", \"Visualization stopped\")\n",
        "        except Exception as e:\n",
        "            logger.critical(f\"Critical error during visualization generation: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Critical error in visualization phase\", str(e))\n",
        "            self.supervisor.update_status(\"failed\", \"Critical visualization error\")\n",
        "\n",
        "\n",
        "    def _generate_category_chart(self):\n",
        "        \"\"\"Generate a bar chart of file categories.\"\"\"\n",
        "        if not PANDAS_AVAILABLE or not MATPLOTLIB_AVAILABLE:\n",
        "            logger.warning(\"Skipping category chart: Pandas or Matplotlib not available.\")\n",
        "            return\n",
        "\n",
        "        query = \"SELECT category, COUNT(*) as count FROM files GROUP BY category ORDER BY count DESC\"\n",
        "        try:\n",
        "            category_data = self.db_manager.execute_query(query)\n",
        "            if not category_data:\n",
        "                logger.warning(\"No file category data found for charting.\")\n",
        "                return\n",
        "\n",
        "            df = pd.DataFrame(category_data)\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            # Use seaborn if available for better aesthetics\n",
        "            if SEABORN_AVAILABLE:\n",
        "                sns.barplot(x='count', y='category', data=df)\n",
        "            else:\n",
        "                plt.barh(df['category'], df['count'])\n",
        "\n",
        "            plt.xlabel(\"Number of Files\")\n",
        "            plt.ylabel(\"Category\")\n",
        "            plt.title(\"Distribution of File Categories\")\n",
        "            plt.tight_layout()\n",
        "            output_path = os.path.join(self.config.visualization_dir, \"Charts\", \"file_category_distribution.png\")\n",
        "            plt.savefig(output_path, dpi=150)\n",
        "            plt.close()\n",
        "            logger.info(f\"File category chart saved to {output_path}\")\n",
        "            self.supervisor.update_stat(\"visualizations_created\", 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate category chart: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Category chart generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"visualization_errors\", 1)\n",
        "\n",
        "\n",
        "    def _generate_filesize_chart(self):\n",
        "        \"\"\"Generate a histogram of file sizes.\"\"\"\n",
        "        if not PANDAS_AVAILABLE or not MATPLOTLIB_AVAILABLE or not NUMPY_AVAILABLE:\n",
        "            logger.warning(\"Skipping file size chart: Pandas, Matplotlib, or NumPy not available.\")\n",
        "            return\n",
        "\n",
        "        # Query file sizes (excluding 0-byte files)\n",
        "        query = \"SELECT filesize FROM files WHERE filesize > 0\"\n",
        "        try:\n",
        "            filesize_data = self.db_manager.execute_query(query)\n",
        "            if not filesize_data:\n",
        "                logger.warning(\"No file size data found for charting.\")\n",
        "                return\n",
        "\n",
        "            sizes = [row['filesize'] for row in filesize_data]\n",
        "            df = pd.DataFrame({'filesize': sizes})\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            # Use log scale for x-axis due to potentially wide range of file sizes\n",
        "            # Use a limited number of bins or specify bin edges\n",
        "            # Consider different plots for different size ranges (e.g., small, medium, large)\n",
        "            # For simplicity, a single histogram with log scale:\n",
        "            if SEABORN_AVAILABLE:\n",
        "                 # Seaborn histplot handles log scale well\n",
        "                 sns.histplot(data=df, x='filesize', bins=50, log_scale=True, kde=True)\n",
        "            else:\n",
        "                 # Matplotlib hist needs manual binning for log scale\n",
        "                 # Define log-spaced bins\n",
        "                 min_size = max(1, min(sizes)) # Ensure min_size is at least 1 for log scale\n",
        "                 max_size = max(sizes)\n",
        "                 if max_size <= min_size:\n",
        "                      logger.warning(\"File sizes are all the same, cannot create histogram.\")\n",
        "                      plt.close() # Close empty figure\n",
        "                      return\n",
        "                 log_bins = np.logspace(np.log10(min_size), np.log10(max_size), 50)\n",
        "                 plt.hist(sizes, bins=log_bins, edgecolor='black')\n",
        "                 plt.xscale('log')\n",
        "\n",
        "\n",
        "            plt.xlabel(\"File Size (Bytes, Log Scale)\")\n",
        "            plt.ylabel(\"Frequency\")\n",
        "            plt.title(\"Distribution of File Sizes\")\n",
        "            plt.tight_layout()\n",
        "            output_path = os.path.join(self.config.visualization_dir, \"Charts\", \"file_size_distribution.png\")\n",
        "            plt.savefig(output_path, dpi=150)\n",
        "            plt.close()\n",
        "            logger.info(f\"File size distribution chart saved to {output_path}\")\n",
        "            self.supervisor.update_stat(\"visualizations_created\", 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate file size chart: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"File size chart generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"visualization_errors\", 1)\n",
        "\n",
        "\n",
        "    def _generate_modification_date_chart(self):\n",
        "        \"\"\"Generate a time series chart of file modification dates.\"\"\"\n",
        "        if not PANDAS_AVAILABLE or not MATPLOTLIB_AVAILABLE:\n",
        "            logger.warning(\"Skipping modification date chart: Pandas or Matplotlib not available.\")\n",
        "            return\n",
        "\n",
        "        # Query modification dates (only non-null dates)\n",
        "        query = \"SELECT modification_date FROM files WHERE modification_date IS NOT NULL\"\n",
        "        try:\n",
        "            date_data = self.db_manager.execute_query(query)\n",
        "            if not date_data:\n",
        "                logger.warning(\"No modification date data found for charting.\")\n",
        "                return\n",
        "\n",
        "            dates = [parse_iso_datetime(row['modification_date']) for row in date_data]\n",
        "            # Filter out dates that failed to parse\n",
        "            valid_dates = [d for d in dates if d is not None]\n",
        "\n",
        "            if not valid_dates:\n",
        "                 logger.warning(\"No valid modification dates found for charting.\")\n",
        "                 return\n",
        "\n",
        "            df = pd.DataFrame({'date': valid_dates})\n",
        "            # Count files per day/week/month depending on the time span\n",
        "            # For simplicity, let's count per month\n",
        "            df['month'] = df['date'].dt.to_period('M')\n",
        "            monthly_counts = df['month'].value_counts().sort_index()\n",
        "\n",
        "            # Convert PeriodIndex to string for plotting\n",
        "            monthly_counts.index = monthly_counts.index.astype(str)\n",
        "\n",
        "            plt.figure(figsize=(15, 8))\n",
        "            if SEABORN_AVAILABLE:\n",
        "                 # Seaborn lineplot\n",
        "                 sns.lineplot(x=monthly_counts.index, y=monthly_counts.values)\n",
        "                 plt.xticks(rotation=45, ha='right') # Rotate labels\n",
        "            else:\n",
        "                 plt.plot(monthly_counts.index, monthly_counts.values)\n",
        "                 plt.xticks(rotation=45, ha='right') # Rotate labels\n",
        "\n",
        "\n",
        "            plt.xlabel(\"Month\")\n",
        "            plt.ylabel(\"Number of Files Modified\")\n",
        "            plt.title(\"File Modification Activity Over Time (Monthly)\")\n",
        "            plt.tight_layout()\n",
        "            output_path = os.path.join(self.config.visualization_dir, \"Charts\", \"file_modification_date_activity.png\")\n",
        "            plt.savefig(output_path, dpi=150)\n",
        "            plt.close()\n",
        "            logger.info(f\"File modification date chart saved to {output_path}\")\n",
        "            self.supervisor.update_stat(\"visualizations_created\", 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate modification date chart: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Modification date chart generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"visualization_errors\", 1)\n",
        "\n",
        "\n",
        "    def _generate_sentiment_distribution_chart(self):\n",
        "        \"\"\"Generate a histogram of message sentiment scores.\"\"\"\n",
        "        if not PANDAS_AVAILABLE or not MATPLOTLIB_AVAILABLE or not SEABORN_AVAILABLE or not SENTIMENT_AVAILABLE:\n",
        "            logger.warning(\"Skipping sentiment distribution chart: Required libraries not available.\")\n",
        "            return\n",
        "\n",
        "        # Get sentiment scores from contacts (average sentiment)\n",
        "        sentiment_scores = [c.sentiment_score for c in self.supervisor.get_state().get('contacts', []) if c.message_count > 0]\n",
        "        if not sentiment_scores:\n",
        "            logger.warning(\"No sentiment data found for charting.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame({'sentiment_score': sentiment_scores})\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Plot histogram with KDE\n",
        "        sns.histplot(data=df, x='sentiment_score', bins=30, kde=True)\n",
        "\n",
        "        plt.xlabel(\"Average Sentiment Score (Compound)\")\n",
        "        plt.ylabel(\"Frequency (Number of Contacts)\")\n",
        "        plt.title(\"Distribution of Average Contact Sentiment Scores\")\n",
        "        plt.xlim(-1, 1) # Sentiment score range\n",
        "        plt.tight_layout()\n",
        "        output_path = os.path.join(self.config.visualization_dir, \"Charts\", \"contact_sentiment_distribution.png\")\n",
        "        plt.savefig(output_path, dpi=150)\n",
        "        plt.close()\n",
        "        logger.info(f\"Contact sentiment distribution chart saved to {output_path}\")\n",
        "        self.supervisor.update_stat(\"visualizations_created\", 1)\n",
        "\n",
        "    def _generate_message_wordcloud(self):\n",
        "        \"\"\"Generate a word cloud from message content.\"\"\"\n",
        "        if not WORDCLOUD_AVAILABLE or not PANDAS_AVAILABLE or not NUMPY_AVAILABLE or not SKLEARN_AVAILABLE:\n",
        "            logger.warning(\"Skipping word cloud: Required libraries not available.\")\n",
        "            return\n",
        "\n",
        "        # Retrieve message content (can be slow for large datasets)\n",
        "        # Option 1: Read from individual conversation files (safer memory-wise)\n",
        "        all_message_texts = []\n",
        "        contact_conv_dir = self.config.contact_conv_dir\n",
        "        if os.path.exists(contact_conv_dir):\n",
        "            logger.info(\"Reading message content from individual contact conversation files for word cloud...\")\n",
        "            for filename in os.listdir(contact_conv_dir):\n",
        "                if stop_event.is_set(): break\n",
        "                if filename.endswith(\".json\"):\n",
        "                    file_path = os.path.join(contact_conv_dir, filename)\n",
        "                    data = load_json_safe(file_path)\n",
        "                    if data:\n",
        "                        for conv_key, conv_data in data.items():\n",
        "                            # Need to get the actual message content, not just metadata\n",
        "                            # This would require re-reading the original message files...\n",
        "                            # For simplicity, let's assume we can get content from the metadata saved earlier\n",
        "                            # This requires a change in MessageAnalyzer to store content persistently or re-read.\n",
        "                            # Let's modify MessageAnalyzer to store content temporarily or query from DB if needed.\n",
        "\n",
        "                            # Alternative: Query content from DB if it was stored (it's not currently)\n",
        "                            # Or, re-read content from the original files listed in conv_data['files']\n",
        "                            # Re-reading original files is the most robust approach if content isn't stored.\n",
        "\n",
        "                            # Let's re-read content from the original files listed in conversation data\n",
        "                            for file_info in conv_data.get('files', []):\n",
        "                                original_file_path = file_info.get('path') # This is just the directory\n",
        "                                original_filename = os.path.basename(file_info.get('path')) # This is incorrect, need filename from file_info\n",
        "                                # Need to query DB to get full path and filename from file_id or stored path\n",
        "                                # Let's assume the 'path' in conv_data['files'] is the full path for now, or query DB.\n",
        "\n",
        "                                # Query DB for full path using file_id if available, or path/filename\n",
        "                                # Assuming file_info in conv_data['files'] has a 'path' key that is the full path\n",
        "                                original_full_path = file_info.get('path') # Assuming this is the full path\n",
        "\n",
        "                                if original_full_path and os.path.exists(original_full_path):\n",
        "                                    try:\n",
        "                                        # Use the same reading logic as _read_message_content\n",
        "                                        content, _ = self._read_message_content(original_full_path) # Ignore metadata\n",
        "                                        if content:\n",
        "                                            all_message_texts.append(content)\n",
        "                                    except Exception as e:\n",
        "                                        logger.warning(f\"Failed to re-read message file {original_full_path} for word cloud: {e}\")\n",
        "                                else:\n",
        "                                     logger.warning(f\"Original message file not found for word cloud: {original_full_path}\")\n",
        "\n",
        "\n",
        "        if stop_event.is_set():\n",
        "             logger.warning(\"Word cloud generation interrupted during file reading.\")\n",
        "             return\n",
        "\n",
        "        if not all_message_texts:\n",
        "            logger.warning(\"No message content found for word cloud generation.\")\n",
        "            return\n",
        "\n",
        "        combined_text = \" \".join(all_message_texts)\n",
        "\n",
        "        try:\n",
        "            # Generate word cloud\n",
        "            wordcloud = WordCloud(width=1600, height=800, random_state=42,\n",
        "                                  background_color='white', colormap='viridis',\n",
        "                                  stopwords=None, # Use default stopwords or provide custom\n",
        "                                  min_font_size=10).generate(combined_text)\n",
        "\n",
        "            # Save the word cloud image\n",
        "            plt.figure(figsize=(16, 8))\n",
        "            plt.imshow(wordcloud, interpolation='bilinear')\n",
        "            plt.axis(\"off\")\n",
        "            plt.tight_layout(pad=0)\n",
        "            output_path = os.path.join(self.config.visualization_dir, \"WordClouds\", \"message_wordcloud.png\")\n",
        "            plt.savefig(output_path, dpi=150)\n",
        "            plt.close()\n",
        "            logger.info(f\"Message word cloud saved to {output_path}\")\n",
        "            self.supervisor.update_stat(\"visualizations_created\", 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate word cloud: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Word cloud generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"visualization_errors\", 1)\n",
        "\n",
        "\n",
        "    def _save_visualization_data(self):\n",
        "        \"\"\"Save data used for visualizations (counts, distributions) to JSON.\"\"\"\n",
        "        logger.info(\"Saving visualization data...\")\n",
        "        viz_data = {}\n",
        "\n",
        "        # File category counts\n",
        "        query_categories = \"SELECT category, COUNT(*) as count FROM files GROUP BY category ORDER BY count DESC\"\n",
        "        try:\n",
        "            viz_data['file_category_counts'] = self.db_manager.execute_query(query_categories)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to retrieve category counts for viz data: {e}\")\n",
        "            viz_data['file_category_counts'] = []\n",
        "\n",
        "        # File size distribution (maybe summary stats or histogram bins)\n",
        "        # For simplicity, just store summary stats for now\n",
        "        query_filesizes = \"SELECT filesize FROM files WHERE filesize > 0\"\n",
        "        try:\n",
        "            filesize_data = self.db_manager.execute_query(query_filesizes)\n",
        "            sizes = [row['filesize'] for row in filesize_data]\n",
        "            if sizes:\n",
        "                viz_data['file_size_stats'] = {\n",
        "                    'count': len(sizes),\n",
        "                    'min': min(sizes),\n",
        "                    'max': max(sizes),\n",
        "                    'mean': np.mean(sizes) if NUMPY_AVAILABLE else None,\n",
        "                    'median': np.median(sizes) if NUMPY_AVAILABLE else None,\n",
        "                    'std_dev': np.std(sizes) if NUMPY_AVAILABLE else None,\n",
        "                }\n",
        "            else:\n",
        "                viz_data['file_size_stats'] = {}\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to retrieve file size data for viz data: {e}\")\n",
        "            viz_data['file_size_stats'] = {}\n",
        "\n",
        "\n",
        "        # Modification date activity (monthly counts)\n",
        "        query_dates = \"SELECT modification_date FROM files WHERE modification_date IS NOT NULL\"\n",
        "        try:\n",
        "            date_data = self.db_manager.execute_query(query_dates)\n",
        "            dates = [parse_iso_datetime(row['modification_date']) for row in date_data]\n",
        "            valid_dates = [d for d in dates if d is not None]\n",
        "            if valid_dates and PANDAS_AVAILABLE:\n",
        "                 df = pd.DataFrame({'date': valid_dates})\n",
        "                 df['month'] = df['date'].dt.to_period('M')\n",
        "                 monthly_counts = df['month'].value_counts().sort_index()\n",
        "                 viz_data['file_modification_monthly_counts'] = monthly_counts.to_dict()\n",
        "            else:\n",
        "                 viz_data['file_modification_monthly_counts'] = {}\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to retrieve modification date data for viz data: {e}\")\n",
        "            viz_data['file_modification_monthly_counts'] = {}\n",
        "\n",
        "\n",
        "        # Sentiment distribution (summary stats or histogram bins)\n",
        "        sentiment_scores = [c.sentiment_score for c in self.supervisor.get_state().get('contacts', []) if c.message_count > 0]\n",
        "        if sentiment_scores and NUMPY_AVAILABLE:\n",
        "             viz_data['contact_sentiment_stats'] = {\n",
        "                 'count': len(sentiment_scores),\n",
        "                 'min': min(sentiment_scores),\n",
        "                 'max': max(sentiment_scores),\n",
        "                 'mean': np.mean(sentiment_scores),\n",
        "                 'median': np.median(sentiment_scores),\n",
        "                 'std_dev': np.std(sentiment_scores),\n",
        "             }\n",
        "        else:\n",
        "             viz_data['contact_sentiment_stats'] = {}\n",
        "\n",
        "\n",
        "        output_path = self.config.visualization_data_path\n",
        "        try:\n",
        "            atomic_write_json(viz_data, output_path)\n",
        "            logger.info(f\"Visualization data saved to {output_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save visualization data JSON: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Failed to save visualization data JSON\", str(e))\n",
        "\n",
        "\n",
        "# --- Report Generator ---\n",
        "\n",
        "class ReportGenerator:\n",
        "    \"\"\"Generates various reports based on analysis results.\"\"\"\n",
        "    def __init__(self, config: Config, db_manager: DatabaseManager, supervisor: AnalysisSupervisor):\n",
        "        self.config = config\n",
        "        self.db_manager = db_manager\n",
        "        self.supervisor = supervisor\n",
        "        # Load contacts and conversations for reporting\n",
        "        self.contacts: List[Contact] = self._load_contacts_for_reporting() or []\n",
        "        self.conversations: Dict[str, Conversation] = self._load_conversations_for_reporting() or {}\n",
        "\n",
        "    def _load_contacts_for_reporting(self) -> Optional[List[Contact]]:\n",
        "         \"\"\"Load contacts from the JSON file for reporting.\"\"\"\n",
        "         # Use the same loading logic as MessageAnalyzer\n",
        "         return MessageAnalyzer(self.config, self.db_manager, self.supervisor)._load_contacts_from_file()\n",
        "\n",
        "    def _load_conversations_for_reporting(self) -> Optional[Dict[str, Conversation]]:\n",
        "         \"\"\"Load conversations from the JSON file for reporting.\"\"\"\n",
        "         # Use the same loading logic as MessageAnalyzer\n",
        "         return MessageAnalyzer(self.config, self.db_manager, self.supervisor)._load_conversations_from_file()\n",
        "\n",
        "\n",
        "    def run_reports(self):\n",
        "        \"\"\"Orchestrate the report generation process.\"\"\"\n",
        "        self.supervisor.update_phase(\"reporting\")\n",
        "        logger.info(\"Starting report generation phase...\")\n",
        "\n",
        "        # Ensure report directory exists\n",
        "        os.makedirs(self.config.report_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # Generate Summary Report\n",
        "            self.supervisor.update_status(\"generating_reports\", \"Generating summary report\")\n",
        "            self._generate_summary_report()\n",
        "\n",
        "            # Generate File Category Report\n",
        "            self.supervisor.update_status(\"generating_reports\", \"Generating file category report\")\n",
        "            self._generate_file_category_report()\n",
        "\n",
        "            # Generate Top Contacts Report\n",
        "            self.supervisor.update_status(\"generating_reports\", \"Generating top contacts report\")\n",
        "            self._generate_top_contacts_report(top_n=self.config.report_top_n)\n",
        "\n",
        "            # Generate Top Conversations Report\n",
        "            self.supervisor.update_status(\"generating_reports\", \"Generating top conversations report\")\n",
        "            self._generate_top_conversations_report(top_n=self.config.report_top_n)\n",
        "\n",
        "            # Generate File Pattern Report\n",
        "            self.supervisor.update_status(\"generating_reports\", \"Generating file pattern report\")\n",
        "            self._generate_file_pattern_report(top_n=self.config.report_top_n)\n",
        "\n",
        "            # Generate Duplicate Files Report\n",
        "            self.supervisor.update_status(\"generating_reports\", \"Generating duplicate files report\")\n",
        "            self._generate_duplicate_files_report()\n",
        "\n",
        "            logger.info(\"Report generation phase completed.\")\n",
        "            self.supervisor.update_status(\"completed\", \"Reports finished\")\n",
        "\n",
        "        except InterruptedError:\n",
        "             logger.warning(\"Report generation phase interrupted by stop signal.\")\n",
        "             self.supervisor.update_status(\"interrupted\", \"Reporting stopped\")\n",
        "        except Exception as e:\n",
        "            logger.critical(f\"Critical error during report generation: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Critical error in reporting phase\", str(e))\n",
        "            self.supervisor.update_status(\"failed\", \"Critical reporting error\")\n",
        "\n",
        "\n",
        "    def _generate_summary_report(self):\n",
        "        \"\"\"Generate a high-level summary report.\"\"\"\n",
        "        logger.info(\"Generating summary report...\")\n",
        "        report_path = os.path.join(self.config.report_dir, \"summary_report.txt\")\n",
        "\n",
        "        state = self.supervisor.get_state()\n",
        "        total_files_in_db = state['stats'].get('files_total_in_db', 0)\n",
        "        total_messages_processed = state['stats'].get('total_messages_processed', 0)\n",
        "        total_contacts = state['stats'].get('contacts_total', 0)\n",
        "        total_conversations = state['stats'].get('conversations_total', 0)\n",
        "        duplicates_marked = state['stats'].get('duplicates_marked', 0)\n",
        "        analysis_start_time = datetime.fromtimestamp(state['startTime']).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        analysis_end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        duration = str(timedelta(seconds=int(time.time() - state['startTime'])))\n",
        "\n",
        "        summary_lines = [\n",
        "            f\"--- Emergency Backup Recovery Analysis Summary Report ---\",\n",
        "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            f\"Script Version: {SCRIPT_VERSION}\",\n",
        "            f\"Backup Directory: {self.config.backup_dir}\",\n",
        "            f\"Output Directory: {self.config.base_output_dir}\",\n",
        "            f\"Analysis Start Time: {analysis_start_time}\",\n",
        "            f\"Analysis End Time: {analysis_end_time}\",\n",
        "            f\"Analysis Duration: {duration}\",\n",
        "            f\"-------------------------------------------------------\",\n",
        "            f\"Total Files Processed (in DB): {total_files_in_db}\",\n",
        "            f\"Files Marked as Duplicates: {duplicates_marked}\",\n",
        "            f\"Total Messages Processed: {total_messages_processed}\",\n",
        "            f\"Total Contacts Identified: {total_contacts}\",\n",
        "            f\"Total Conversations Identified (after filtering): {len(self.conversations)} / {total_conversations} (filtered)\",\n",
        "            f\"-------------------------------------------------------\",\n",
        "            f\"Key Statistics:\",\n",
        "        ]\n",
        "\n",
        "        # Add key stats from supervisor state\n",
        "        for key, value in state['stats'].items():\n",
        "             if key not in ['files_total_in_db', 'total_messages_processed', 'contacts_total', 'conversations_total', 'duplicates_marked', 'conversations_filtered_out']:\n",
        "                 summary_lines.append(f\"  {key}: {value}\")\n",
        "\n",
        "        summary_lines.append(\"-------------------------------------------------------\")\n",
        "        summary_lines.append(\"Errors and Warnings:\")\n",
        "        if state['errors']:\n",
        "            summary_lines.append(f\"  Errors ({len(state['errors'])}):\")\n",
        "            for error in state['errors']:\n",
        "                summary_lines.append(f\"    - [{error['timestampFormatted']}] {error['message']} (Detail: {error['detail']})\")\n",
        "        else:\n",
        "            summary_lines.append(\"  No errors reported.\")\n",
        "\n",
        "        if state['warnings']:\n",
        "            summary_lines.append(f\"  Warnings ({len(state['warnings'])}):\")\n",
        "            for warning in state['warnings']:\n",
        "                summary_lines.append(f\"    - [{warning['timestampFormatted']}] {warning['message']} (Detail: {warning['detail']})\")\n",
        "        else:\n",
        "            summary_lines.append(\"  No warnings reported.\")\n",
        "\n",
        "        summary_lines.append(\"-------------------------------------------------------\")\n",
        "        summary_lines.append(\"See other reports and visualizations for detailed analysis.\")\n",
        "        summary_lines.append(\"--- End of Summary Report ---\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            with open(report_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"\\n\".join(summary_lines))\n",
        "            logger.info(f\"Summary report saved to {report_path}\")\n",
        "            self.supervisor.update_stat(\"reports_generated\", 1)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save summary report: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Summary report generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"report_errors\", 1)\n",
        "\n",
        "\n",
        "    def _generate_file_category_report(self):\n",
        "        \"\"\"Generate a report on file categories.\"\"\"\n",
        "        logger.info(\"Generating file category report...\")\n",
        "        report_path = os.path.join(self.config.report_dir, \"file_category_report.txt\")\n",
        "\n",
        "        query = \"SELECT category, COUNT(*) as count, SUM(filesize) as total_size FROM files GROUP BY category ORDER BY count DESC\"\n",
        "        try:\n",
        "            category_data = self.db_manager.execute_query(query)\n",
        "\n",
        "            report_lines = [\n",
        "                \"--- File Category Report ---\",\n",
        "                f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "                \"----------------------------\",\n",
        "                \"File Counts and Total Sizes by Category:\",\n",
        "                \"\"\n",
        "            ]\n",
        "\n",
        "            if category_data:\n",
        "                headers = [\"Category\", \"File Count\", \"Total Size\"]\n",
        "                table_data = []\n",
        "                for row in category_data:\n",
        "                    table_data.append([row['category'], row['count'], format_bytes(row['total_size'] or 0)])\n",
        "\n",
        "                if TABULATE_AVAILABLE:\n",
        "                    report_lines.append(tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
        "                else:\n",
        "                    # Basic text formatting if tabulate is not available\n",
        "                    report_lines.append(\" | \".join(headers))\n",
        "                    report_lines.append(\"-\" * (len(\" | \".join(headers)) + 5)) # Simple separator\n",
        "                    for row in table_data:\n",
        "                         report_lines.append(\" | \".join(map(str, row)))\n",
        "\n",
        "            else:\n",
        "                report_lines.append(\"No file category data available.\")\n",
        "\n",
        "            report_lines.append(\"\\n----------------------------\")\n",
        "            report_lines.append(\"--- End of File Category Report ---\")\n",
        "\n",
        "            with open(report_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"\\n\".join(report_lines))\n",
        "            logger.info(f\"File category report saved to {report_path}\")\n",
        "            self.supervisor.update_stat(\"reports_generated\", 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate file category report: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"File category report generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"report_errors\", 1)\n",
        "\n",
        "\n",
        "    def _generate_top_contacts_report(self, top_n: int = DEFAULT_REPORT_TOP_N):\n",
        "        \"\"\"Generate a report of top contacts by message count.\"\"\"\n",
        "        logger.info(f\"Generating top {top_n} contacts report...\")\n",
        "        report_path = os.path.join(self.config.report_dir, \"top_contacts_report.txt\")\n",
        "\n",
        "        # Use the loaded contacts, which are already sorted by message count\n",
        "        top_contacts = self.contacts[:top_n]\n",
        "\n",
        "        report_lines = [\n",
        "            f\"--- Top {len(top_contacts)} Contacts Report (by Message Count) ---\",\n",
        "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            \"-----------------------------------------------------\",\n",
        "            \"\"\n",
        "        ]\n",
        "\n",
        "        if top_contacts:\n",
        "            headers = [\"Rank\", \"Phone Number\", \"Messages\", \"Sentiment\", \"First Seen\", \"Last Seen\", \"Topics\"]\n",
        "            table_data = []\n",
        "            for i, contact in enumerate(top_contacts):\n",
        "                table_data.append([\n",
        "                    i + 1,\n",
        "                    contact.phone_number,\n",
        "                    contact.message_count,\n",
        "                    f\"{contact.sentiment_score:.2f}\" if SENTIMENT_AVAILABLE else \"N/A\",\n",
        "                    contact.first_seen or \"N/A\",\n",
        "                    contact.last_seen or \"N/A\",\n",
        "                    \", \".join(contact.topics) if contact.topics else \"N/A\"\n",
        "                ])\n",
        "\n",
        "            if TABULATE_AVAILABLE:\n",
        "                report_lines.append(tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
        "            else:\n",
        "                # Basic text formatting\n",
        "                report_lines.append(\" | \".join(headers))\n",
        "                report_lines.append(\"-\" * (len(\" | \".join(headers)) + 5))\n",
        "                for row in table_data:\n",
        "                     report_lines.append(\" | \".join(map(str, row)))\n",
        "\n",
        "        else:\n",
        "            report_lines.append(\"No contact data available.\")\n",
        "\n",
        "        report_lines.append(\"\\n-----------------------------------------------------\")\n",
        "        report_lines.append(\"--- End of Top Contacts Report ---\")\n",
        "\n",
        "        try:\n",
        "            with open(report_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"\\n\".join(report_lines))\n",
        "            logger.info(f\"Top contacts report saved to {report_path}\")\n",
        "            self.supervisor.update_stat(\"reports_generated\", 1)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate top contacts report: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Top contacts report generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"report_errors\", 1)\n",
        "\n",
        "\n",
        "    def _generate_top_conversations_report(self, top_n: int = DEFAULT_REPORT_TOP_N):\n",
        "        \"\"\"Generate a report of top conversations by message count.\"\"\"\n",
        "        logger.info(f\"Generating top {top_n} conversations report...\")\n",
        "        report_path = os.path.join(self.config.report_dir, \"top_conversations_report.txt\")\n",
        "\n",
        "        # Sort conversations by message count\n",
        "        sorted_conversations = sorted(self.conversations.values(), key=lambda c: c.message_count, reverse=True)[:top_n]\n",
        "\n",
        "        report_lines = [\n",
        "            f\"--- Top {len(sorted_conversations)} Conversations Report (by Message Count) ---\",\n",
        "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            \"----------------------------------------------------------\",\n",
        "            \"\"\n",
        "        ]\n",
        "\n",
        "        if sorted_conversations:\n",
        "            headers = [\"Rank\", \"Participants\", \"Messages\", \"Sentiment\", \"First Message\", \"Last Message\", \"Type\", \"Topics\"]\n",
        "            table_data = []\n",
        "            for i, conversation in enumerate(sorted_conversations):\n",
        "                table_data.append([\n",
        "                    i + 1,\n",
        "                    \", \".join(conversation.participants),\n",
        "                    conversation.message_count,\n",
        "                    f\"{conversation.sentiment_score:.2f}\" if SENTIMENT_AVAILABLE else \"N/A\",\n",
        "                    conversation.first_message or \"N/A\",\n",
        "                    conversation.last_message or \"N/A\",\n",
        "                    \"Group\" if conversation.is_group_chat else \"Private\",\n",
        "                    \", \".join(conversation.topics) if conversation.topics else \"N/A\"\n",
        "                ])\n",
        "\n",
        "            if TABULATE_AVAILABLE:\n",
        "                report_lines.append(tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
        "            else:\n",
        "                # Basic text formatting\n",
        "                report_lines.append(\" | \".join(headers))\n",
        "                report_lines.append(\"-\" * (len(\" | \".join(headers)) + 5))\n",
        "                for row in table_data:\n",
        "                     report_lines.append(\" | \".join(map(str, row)))\n",
        "\n",
        "        else:\n",
        "            report_lines.append(\"No conversation data available (check min_conversation_files setting).\")\n",
        "\n",
        "        report_lines.append(\"\\n----------------------------------------------------------\")\n",
        "        report_lines.append(\"--- End of Top Conversations Report ---\")\n",
        "\n",
        "        try:\n",
        "            with open(report_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"\\n\".join(report_lines))\n",
        "            logger.info(f\"Top conversations report saved to {report_path}\")\n",
        "            self.supervisor.update_stat(\"reports_generated\", 1)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate top conversations report: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Top conversations report generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"report_errors\", 1)\n",
        "\n",
        "\n",
        "    def _generate_file_pattern_report(self, top_n: int = DEFAULT_REPORT_TOP_N):\n",
        "        \"\"\"Generate a report on frequently occurring file patterns.\"\"\"\n",
        "        logger.info(f\"Generating top {top_n} file patterns report...\")\n",
        "        report_path = os.path.join(self.config.report_dir, \"file_pattern_report.txt\")\n",
        "\n",
        "        query = \"SELECT pattern_type, pattern_value, COUNT(*) as count FROM patterns GROUP BY pattern_type, pattern_value ORDER BY count DESC\"\n",
        "        try:\n",
        "            pattern_data = self.db_manager.execute_query(query)\n",
        "\n",
        "            report_lines = [\n",
        "                f\"--- Top File Patterns Report (Top {top_n} per Type) ---\",\n",
        "                f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "                \"----------------------------------------------------\",\n",
        "                \"\"\n",
        "            ]\n",
        "\n",
        "            if pattern_data:\n",
        "                patterns_by_type = defaultdict(list)\n",
        "                for row in pattern_data:\n",
        "                    patterns_by_type[row['pattern_type']].append((row['pattern_value'], row['count']))\n",
        "\n",
        "                for p_type, patterns in patterns_by_type.items():\n",
        "                    report_lines.append(f\"Pattern Type: {p_type}\")\n",
        "                    headers = [\"Rank\", \"Pattern Value\", \"Count\"]\n",
        "                    table_data = []\n",
        "                    for i, (value, count) in enumerate(patterns[:top_n]):\n",
        "                         table_data.append([i + 1, value, count])\n",
        "\n",
        "                    if TABULATE_AVAILABLE:\n",
        "                        report_lines.append(tabulate(table_data, headers=headers, tablefmt=\"plain\"))\n",
        "                    else:\n",
        "                        report_lines.append(\" | \".join(headers))\n",
        "                        report_lines.append(\"-\" * (len(\" | \".join(headers)) + 5))\n",
        "                        for row in table_data:\n",
        "                             report_lines.append(\" | \".join(map(str, row)))\n",
        "\n",
        "                    report_lines.append(\"\") # Add a blank line between types\n",
        "\n",
        "            else:\n",
        "                report_lines.append(\"No file pattern data available.\")\n",
        "\n",
        "            report_lines.append(\"\\n----------------------------------------------------\")\n",
        "            report_lines.append(\"--- End of File Patterns Report ---\")\n",
        "\n",
        "            with open(report_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"\\n\".join(report_lines))\n",
        "            logger.info(f\"File patterns report saved to {report_path}\")\n",
        "            self.supervisor.update_stat(\"reports_generated\", 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate file pattern report: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"File pattern report generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"report_errors\", 1)\n",
        "\n",
        "\n",
        "    def _generate_duplicate_files_report(self):\n",
        "        \"\"\"Generate a report listing duplicate files.\"\"\"\n",
        "        logger.info(\"Generating duplicate files report...\")\n",
        "        report_path = os.path.join(self.config.report_dir, \"duplicate_files_report.txt\")\n",
        "\n",
        "        # Query files marked as duplicates, grouped by original file\n",
        "        query = \"\"\"\n",
        "            SELECT\n",
        "                f.md5hash,\n",
        "                orig.path AS original_path,\n",
        "                orig.filename AS original_filename,\n",
        "                COUNT(f.id) AS duplicate_count\n",
        "            FROM files f\n",
        "            JOIN files orig ON f.original_id = orig.id\n",
        "            WHERE f.is_duplicate = 1\n",
        "            GROUP BY f.original_id, f.md5hash -- Group by original and hash for clarity\n",
        "            ORDER BY duplicate_count DESC, f.md5hash ASC\n",
        "        \"\"\"\n",
        "        try:\n",
        "            duplicate_groups = self.db_manager.execute_query(query)\n",
        "\n",
        "            report_lines = [\n",
        "                \"--- Duplicate Files Report ---\",\n",
        "                f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "                \"------------------------------\",\n",
        "                \"\"\n",
        "            ]\n",
        "\n",
        "            if duplicate_groups:\n",
        "                report_lines.append(\"Groups of duplicate files (grouped by original file and hash):\")\n",
        "                report_lines.append(\"\")\n",
        "\n",
        "                for group in duplicate_groups:\n",
        "                    md5 = group['md5hash']\n",
        "                    original_path = group['original_path']\n",
        "                    original_filename = group['original_filename']\n",
        "                    duplicate_count = group['duplicate_count']\n",
        "\n",
        "                    report_lines.append(f\"MD5 Hash: {md5}\")\n",
        "                    report_lines.append(f\"Original File: {os.path.join(original_path, original_filename)}\")\n",
        "                    report_lines.append(f\"Number of Duplicates: {duplicate_count}\")\n",
        "\n",
        "                    # List the duplicate files in this group\n",
        "                    query_duplicates = \"\"\"\n",
        "                        SELECT path, filename\n",
        "                        FROM files\n",
        "                        WHERE original_id = ? AND md5hash = ? AND is_duplicate = 1\n",
        "                        ORDER BY path ASC, filename ASC\n",
        "                    \"\"\"\n",
        "                    duplicates_list = self.db_manager.execute_query(query_duplicates, (group['original_id'], md5))\n",
        "\n",
        "                    if duplicates_list:\n",
        "                        report_lines.append(\"Duplicate File Paths:\")\n",
        "                        for dup_row in duplicates_list:\n",
        "                            report_lines.append(f\"  - {os.path.join(dup_row['path'], dup_row['filename'])}\")\n",
        "                    report_lines.append(\"-\" * 30) # Separator between groups\n",
        "\n",
        "            else:\n",
        "                report_lines.append(\"No duplicate files found.\")\n",
        "\n",
        "            report_lines.append(\"\\n------------------------------\")\n",
        "            report_lines.append(\"--- End of Duplicate Files Report ---\")\n",
        "\n",
        "            with open(report_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"\\n\".join(report_lines))\n",
        "            logger.info(f\"Duplicate files report saved to {report_path}\")\n",
        "            self.supervisor.update_stat(\"reports_generated\", 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to generate duplicate files report: {e}\", exc_info=True)\n",
        "            self.supervisor.log_error(\"Duplicate files report generation failed\", str(e))\n",
        "            self.supervisor.update_stat(\"report_errors\", 1)\n",
        "\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to parse arguments and run the analysis pipeline.\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Consolidated Analysis Script for Emergency Backup Recovery\")\n",
        "    parser.add_argument(\"backup_dir\", help=\"Path to the emergency backup directory.\")\n",
        "    parser.add_argument(\"output_dir\", help=\"Path to the output directory for results, logs, and database.\")\n",
        "    parser.add_argument(\"--sample_rate\", type=float, default=1.0,\n",
        "                        help=\"Fraction of files to sample for analysis (0.0 to 1.0). Default: 1.0 (all files).\")\n",
        "    parser.add_argument(\"--max_files\", type=int, default=None,\n",
        "                        help=\"Maximum number of files to process. Default: None (no limit).\")\n",
        "    parser.add_argument(\"--resume\", action=\"store_true\",\n",
        "                        help=\"Resume analysis from the last checkpoint.\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=DEFAULT_BATCH_SIZE,\n",
        "                        help=f\"Number of files to process in each batch for database insertion. Default: {DEFAULT_BATCH_SIZE}.\")\n",
        "    parser.add_argument(\"--min_conversation_files\", type=int, default=DEFAULT_MIN_CONVERSATION_FILES,\n",
        "                        help=f\"Minimum number of files required to consider a conversation significant for reporting. Default: {DEFAULT_MIN_CONVERSATION_FILES}.\")\n",
        "    parser.add_argument(\"--parallel_jobs\", type=int, default=os.cpu_count() or 1,\n",
        "                        help=f\"Number of parallel processes/threads to use. Default: Number of CPU cores ({os.cpu_count() or 1}).\")\n",
        "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging.\")\n",
        "    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Suppress informational output (only show warnings/errors).\")\n",
        "    parser.add_argument(\"--report_top_n\", type=int, default=DEFAULT_REPORT_TOP_N,\n",
        "                        help=f\"Number of top items to include in reports (e.g., top contacts, patterns). Default: {DEFAULT_REPORT_TOP_N}.\")\n",
        "    parser.add_argument(\"--graph_top_n\", type=int, default=DEFAULT_GRAPH_TOP_N,\n",
        "                        help=f\"Number of top nodes to include in graph visualizations. Default: {DEFAULT_GRAPH_TOP_N}.\")\n",
        "\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Determine debug level based on arguments\n",
        "    debug_level = 1 # Default: INFO\n",
        "    if args.debug:\n",
        "        debug_level = 2 # DEBUG\n",
        "    elif args.quiet:\n",
        "        debug_level = 0 # WARNING\n",
        "\n",
        "    # Create configuration object\n",
        "    config = Config(\n",
        "        backup_dir=args.backup_dir,\n",
        "        base_output_dir=args.output_dir,\n",
        "        sample_rate=args.sample_rate,\n",
        "        max_files=args.max_files,\n",
        "        resume=args.resume,\n",
        "        batch_size=args.batch_size,\n",
        "        min_conversation_files=args.min_conversation_files,\n",
        "        parallel_jobs=args.parallel_jobs,\n",
        "        debug_level=debug_level,\n",
        "        report_top_n=args.report_top_n,\n",
        "        graph_top_n=args.graph_top_n,\n",
        "    )\n",
        "\n",
        "    # Create necessary directories\n",
        "    try:\n",
        "        config.create_directories()\n",
        "    except Exception as e:\n",
        "        print(f\"FATAL: Could not create output directories: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Setup logging\n",
        "    setup_logging(config.log_dir, config.debug_level)\n",
        "\n",
        "    # Register signal handlers for graceful shutdown\n",
        "    signal.signal(signal.SIGINT, signal_handler)\n",
        "    signal.signal(signal.SIGTERM, signal_handler)\n",
        "    # Add other signals if needed (e.g., SIGQUIT)\n",
        "    if platform.system() != \"Windows\":\n",
        "         try:\n",
        "             signal.signal(signal.SIGQUIT, signal_handler)\n",
        "         except AttributeError:\n",
        "             pass # SIGQUIT is not available on all platforms\n",
        "\n",
        "\n",
        "    # Initialize Supervisor, DB Manager, and Analyzers\n",
        "    supervisor = None\n",
        "    db_manager = None\n",
        "    file_analyzer = None\n",
        "    message_analyzer = None\n",
        "    visualization_generator = None\n",
        "    report_generator = None\n",
        "\n",
        "    try:\n",
        "        # Check if another instance is running using PID file\n",
        "        if os.path.exists(config.pid_file_path):\n",
        "            try:\n",
        "                with open(config.pid_file_path, 'r') as f:\n",
        "                    pid_in_file = int(f.read().strip())\n",
        "                # Check if the process with that PID is still running\n",
        "                if os.path.exists(f\"/proc/{pid_in_file}\"): # Linux specific check, needs platform adaptation\n",
        "                     logger.error(f\"Another instance of the script is already running with PID {pid_in_file}. Exiting.\")\n",
        "                     sys.exit(1)\n",
        "                else:\n",
        "                     logger.warning(f\"Stale PID file found for PID {pid_in_file}. Removing.\")\n",
        "                     os.remove(config.pid_file_path)\n",
        "            except (ValueError, FileNotFoundError, OSError) as e:\n",
        "                logger.warning(f\"Could not check/clean up PID file {config.pid_file_path}: {e}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Unexpected error during PID file check: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "        supervisor = AnalysisSupervisor(config)\n",
        "        db_manager = DatabaseManager(config.db_path)\n",
        "        file_analyzer = FileAnalyzer(config, db_manager, supervisor)\n",
        "        message_analyzer = MessageAnalyzer(config, db_manager, supervisor) # Needs contacts/conversations loaded\n",
        "        visualization_generator = VisualizationGenerator(config, db_manager, supervisor)\n",
        "        report_generator = ReportGenerator(config, db_manager, supervisor) # Needs contacts/conversations loaded\n",
        "\n",
        "        supervisor.update_status(\"running\", \"Starting analysis pipeline\")\n",
        "\n",
        "        # --- Analysis Pipeline Stages ---\n",
        "\n",
        "        # 1. File Analysis (Scan, Analyze, Deduplicate)\n",
        "        if not stop_event.is_set():\n",
        "            file_analyzer.run_file_analysis()\n",
        "        else:\n",
        "            logger.warning(\"Skipping file analysis phase due to stop signal.\")\n",
        "\n",
        "\n",
        "        # 2. Message Analysis (Extract contacts/conversations, sentiment, topics)\n",
        "        # Message analysis depends on files being categorized in the DB\n",
        "        if not stop_event.is_set():\n",
        "            message_analyzer.run_message_analysis()\n",
        "        else:\n",
        "            logger.warning(\"Skipping message analysis phase due to stop signal.\")\n",
        "\n",
        "\n",
        "        # 3. Visualization Generation\n",
        "        # Visualizations depend on analysis results (DB, JSON files)\n",
        "        if not stop_event.is_set():\n",
        "            visualization_generator.run_visualizations()\n",
        "        else:\n",
        "            logger.warning(\"Skipping visualization phase due to stop signal.\")\n",
        "\n",
        "\n",
        "        # 4. Report Generation\n",
        "        # Reports depend on analysis results (DB, JSON files)\n",
        "        if not stop_event.is_set():\n",
        "            report_generator.run_reports()\n",
        "        else:\n",
        "            logger.warning(\"Skipping reporting phase due to stop signal.\")\n",
        "\n",
        "\n",
        "        # --- Finalization ---\n",
        "        if stop_event.is_set():\n",
        "             supervisor.update_status(\"interrupted\", \"Analysis stopped by user/signal\")\n",
        "             logger.warning(\"Analysis pipeline interrupted.\")\n",
        "        else:\n",
        "             supervisor.update_status(\"completed\", \"Analysis pipeline finished successfully\")\n",
        "             logger.info(\"Analysis pipeline completed successfully.\")\n",
        "\n",
        "    except InterruptedError:\n",
        "        logger.warning(\"Analysis pipeline interrupted by stop signal.\")\n",
        "        if supervisor:\n",
        "             supervisor.update_status(\"interrupted\", \"Analysis stopped by user/signal\")\n",
        "        # Cleanup will happen in finally block\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"A critical error occurred during the analysis pipeline: {e}\", exc_info=True)\n",
        "        if supervisor:\n",
        "            supervisor.log_error(\"Critical error in pipeline\", str(e))\n",
        "            supervisor.update_status(\"failed\", \"Critical pipeline error\")\n",
        "        sys.exit(1) # Exit with error code\n",
        "    finally:\n",
        "        # Ensure resources are closed\n",
        "        if db_manager:\n",
        "            db_manager.close()\n",
        "        if supervisor:\n",
        "            supervisor.close()\n",
        "        logger.info(\"Analysis script finished.\")\n",
        "\n",
        "\n",
        "# --- Script Entry Point ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ytca-YEsigpU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}